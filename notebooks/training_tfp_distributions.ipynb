{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19efe82f-b54f-4c60-b3a6-05eaadb0e47d",
   "metadata": {},
   "source": [
    "# Training TensorFlow Probability distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31974d2-81cb-4f0f-b589-47ee8e674bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9f527-4e5e-4379-8a90-e11be9e2ac69",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c12664-b0a8-45c1-9655-992333750ea8",
   "metadata": {},
   "source": [
    "Generate sythetic data from a chosen probability distribution and then fit another distribution on the data, training its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a3341-3996-47e9-b0dd-41c6335fd12f",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3df6b2-318d-4288-8cad-63d5e585d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_loc = 3.3\n",
    "generating_scale = 2.1\n",
    "\n",
    "generating_distr = tfd.Normal(loc=generating_loc, scale=generating_scale)\n",
    "\n",
    "generating_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc3b04-d696-4991-b0be-df1d47083e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 5000\n",
    "\n",
    "samples = generating_distr.sample(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d65a6-568c-45db-a794-ee3a5c9e6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Histogram of samples'\n",
    ")\n",
    "\n",
    "grid_points = tf.linspace(-5., 10., 1000)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=grid_points,\n",
    "    y=generating_distr.prob(grid_points),\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generating PDF'\n",
    ")\n",
    "\n",
    "plt.xlabel('Values', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(loc='upper right', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d12786-edfd-469b-af51-d8c9e0bf8f21",
   "metadata": {},
   "source": [
    "## Train a distribution on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1adc-9a84-4262-a71f-6d6cf1ed7beb",
   "metadata": {},
   "source": [
    "**Enforcing contraints on the parameters:** some probability distributions have parameters that are required to obey constraints (e.g. they must be positive, add up to 1, etc.). These constraints should be satisfied at each step of the optimization procedure and there exist vaious methods to make this happen, among which:\n",
    "- \"Clipping\" negative values to 0 or to a small positive number $\\epsilon$: every time the optimizer generates a negative number this is mapped to either 0 or $\\epsilon$, depending on what's required.\n",
    "- Pass values through a function that maps them to positive values.\n",
    "- Use an optimizer that supports constraints naturally (e.g. LBFGS).\n",
    "\n",
    "In this notebook we implemented the first and the second options (which one is used can be chosen by the user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a26204-4eac-4f70-80e5-aa6860a23b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(samples, distr):\n",
    "    \"\"\"\n",
    "    Negative log likelihood for a given dataset according\n",
    "    to a given distribution. This will be used as the loss\n",
    "    function during the optimization phase.\n",
    "    \"\"\"\n",
    "    return - tf.reduce_mean(distr.log_prob(samples))\n",
    "\n",
    "@tf.function\n",
    "def get_loss_and_grads(samples, loss, distr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(distr.trainable_variables)\n",
    "        \n",
    "        # Compute the value of the loss at the current point in\n",
    "        # parameters' space.\n",
    "        loss_value = loss(samples, distr)\n",
    "        \n",
    "        # Compute the gradient of the loss function at the current\n",
    "        # point in parameters' space.\n",
    "        grads = tape.gradient(loss_value, distr.trainable_variables)\n",
    "\n",
    "    return loss_value, grads\n",
    "\n",
    "\n",
    "def distr_optimization(\n",
    "    samples,\n",
    "    loss,\n",
    "    distr,\n",
    "    param_names,\n",
    "    n_iter=500,\n",
    "    learning_rate=0.2,\n",
    "    positive_params=[],\n",
    "    positivity_constraint_method='clip'\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    missing_params = [\n",
    "        p for p in positive_params\n",
    "        if p not in param_names\n",
    "    ]\n",
    "    \n",
    "    if len(missing_params) > 0:\n",
    "        raise Exception(\n",
    "            f\"Some parameters required to be positive are not mentioned among the parameters' names: {missing_params}\")\n",
    "    \n",
    "    loss_history = []\n",
    "    grad_history = []\n",
    "    params_history = []\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        loss_value, grads = get_loss_and_grads(samples, loss, distr)\n",
    "        \n",
    "        loss_history.append(loss_value)\n",
    "        grad_history.append(grads)\n",
    "        \n",
    "        params_history.append([\n",
    "            distr.parameters[param].numpy() for param in param_names\n",
    "        ])\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, distr.trainable_variables))\n",
    "        \n",
    "        # For parameters that must be kept positive at each iteration,\n",
    "        # we clip the values to a small positive value.\n",
    "        if positivity_constraint_method == 'clip':\n",
    "            eps = 1e-6\n",
    "\n",
    "            for p in positive_params:\n",
    "                # print(f'Clipping value for parameter: {p}')\n",
    "                \n",
    "                distr.parameters[p] = tf.maximum(distr.parameters[p], eps)\n",
    "        elif positivity_constraint_method == 'exp':\n",
    "            for p in positive_params:\n",
    "                # print(f'Applying an exponential function to parameter: {p}')\n",
    "                \n",
    "                distr.parameters[p] = tf.exp(distr.parameters[p])\n",
    "        else:\n",
    "            raise Exception(f'Positivity constraint method {positivity_constraint_method} not available')\n",
    "        \n",
    "    # Record the final values.\n",
    "    loss_value, grads = get_loss_and_grads(samples, loss, distr)\n",
    "        \n",
    "    loss_history.append(loss_value)\n",
    "    grad_history.append(grads)\n",
    "    \n",
    "    params_history.append([\n",
    "        distr.parameters[param].numpy() for param in param_names\n",
    "    ])\n",
    "    \n",
    "    loss_history = tf.concat(loss_history, axis=0)\n",
    "    grad_history = tf.concat(grad_history, axis=0)\n",
    "    params_history = tf.stack(params_history, axis=1)\n",
    "    \n",
    "    return loss_history, grad_history, params_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d49f0-c525-42a7-856c-3708af82adf3",
   "metadata": {},
   "source": [
    "Define a trainable distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678eb512-cb56-4c81-a04d-041f3493c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainable parameters must be TensorFlow variables.\n",
    "trainable_loc = tf.Variable(0.0, name='trainable_loc')\n",
    "trainable_scale = tf.Variable(1.0, name='trainable_scale')\n",
    "\n",
    "trainable_distr = tfd.Normal(loc=trainable_loc, scale=trainable_scale)\n",
    "\n",
    "trainable_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe090f3e-30f7-4cde-befe-227b6a0d1b95",
   "metadata": {},
   "source": [
    "Plot the samples, the generating PDF and the PDF corresponding to the trainable distribution with the initial values for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96df6f0-69d4-4f7a-b9c6-c9e8b4e37356",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Histogram of samples'\n",
    ")\n",
    "\n",
    "grid_points = tf.linspace(-5., 10., 1000)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=grid_points,\n",
    "    y=generating_distr.prob(grid_points),\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generating PDF'\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=grid_points,\n",
    "    y=trainable_distr.prob(grid_points),\n",
    "    color=sns.color_palette()[3],\n",
    "    label='Trainable PDF (initial values for parameters)'\n",
    ")\n",
    "\n",
    "plt.xlabel('Values', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(loc='upper right', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a3a17-daad-4ff1-8af9-fb81d61a383c",
   "metadata": {},
   "source": [
    "Define the tools for the optimization phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749806-c191-4b8d-8ac6-f79d9cf9b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history, grad_history, params_history = distr_optimization(\n",
    "    samples,\n",
    "    nll,\n",
    "    trainable_distr,\n",
    "    param_names=['loc', 'scale'],\n",
    "    n_iter=500,\n",
    "    learning_rate=.2,\n",
    "    positive_params=['scale'],\n",
    "    positivity_constraint_method='exp'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'Initial loss: {loss_history[0]}\\n'\n",
    "    f'Final loss: {loss_history[-1]}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d84b21-6f82-45ab-8c42-fe562fb589ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(14, 15), sharex=True)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history)),\n",
    "    y=loss_history.numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.ylabel('Loss values', fontsize=12)\n",
    "plt.title('Optimization history', fontsize=16)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history.shape[1]),\n",
    "    y=params_history[0, :].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history.shape[1]),\n",
    "    y=np.ones(params_history.shape[1]) * generating_loc,\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.ylabel('Loc param values', fontsize=12)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history.shape[1]),\n",
    "    y=params_history[1, :].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[2]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history.shape[1]),\n",
    "    y=np.ones(params_history.shape[1]) * generating_scale,\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[2]\n",
    ")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.xlabel('N iteration', fontsize=12)\n",
    "plt.ylabel('Scale param values', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b36db5-6812-4833-9e95-42bf324014a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_error = tf.abs(trainable_distr.loc - generating_loc) / generating_loc\n",
    "scale_error = tf.abs(trainable_distr.scale - generating_scale) / generating_scale\n",
    "\n",
    "print(\n",
    "    f'Error on loc parameter: {loc_error}\\n'\n",
    "    f'Error on scale parameter: {scale_error}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efddb94-2097-439e-a313-706e77792e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Histogram of samples'\n",
    ")\n",
    "\n",
    "grid_points = tf.linspace(-5., 10., 1000)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=grid_points,\n",
    "    y=generating_distr.prob(grid_points),\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generating PDF'\n",
    ")\n",
    "\n",
    "params_to_plot = tf.concat([params_history[:, ::50], params_history[:, -1, tf.newaxis]], axis=1)\n",
    "\n",
    "for i in range(params_to_plot.shape[1]):\n",
    "    distr_to_plot = tfd.Normal(\n",
    "        loc=params_to_plot[0, i],\n",
    "        scale=params_to_plot[1, i]\n",
    "    )\n",
    "    \n",
    "    sns.lineplot(\n",
    "        x=grid_points,\n",
    "        y=distr_to_plot.prob(grid_points),\n",
    "        color=sns.color_palette()[3],\n",
    "        label=f'Trainable PDF (step {i+1})',\n",
    "        alpha=0.4\n",
    "    )\n",
    "\n",
    "plt.xlabel('Values', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(loc='upper right', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f167f4ed-0f10-48c9-9e6f-a408f1afe665",
   "metadata": {},
   "source": [
    "## An example in 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa34c3-cf8c-4baf-9c96-8aaa2cebe95b",
   "metadata": {},
   "source": [
    "Here we use a 2x2 covariance matrix that is diagonal by construction. In the most general case, we should use a full covariance matrix with an optimization procedure that makes sure that at each iteration the covariance matrix itself stays symmetric and positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8eb0f-7a1c-4dad-a941-903741b8fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_loc_2d = [3.3, 4.1]\n",
    "generating_scale_2d = [2.1, 1.1]\n",
    "\n",
    "generating_distr_2d = tfd.MultivariateNormalDiag(loc=generating_loc_2d, scale_diag=generating_scale_2d)\n",
    "\n",
    "generating_distr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8651736-d8fd-4e29-8012-b263ab8c5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_2d = generating_distr_2d.sample(n_samples)\n",
    "\n",
    "samples_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92a822-4e4d-4ee8-af15-c41a9ddaa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples_2d[:, 0].numpy(),\n",
    "    y=samples_2d[:, 1].numpy()\n",
    ")\n",
    "# g.plot_joint(sns.kdeplot, color=\"r\", zorder=1, levels=6)\n",
    "\n",
    "\n",
    "xs, ys = np.meshgrid(\n",
    "    np.linspace(-5., 12.5, 100),\n",
    "    np.linspace(0., 8.5, 100)\n",
    ")\n",
    "\n",
    "grid_points = np.stack(\n",
    "    [xs.ravel(), ys.ravel()]\n",
    ").T\n",
    "\n",
    "plt.contour(\n",
    "    xs,\n",
    "    ys,\n",
    "    generating_distr_2d.prob(grid_points).numpy().reshape(100, 100),\n",
    "    levels=20,\n",
    ")\n",
    "\n",
    "plt.title('Generated points', fontsize=16)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec028d5a-cff0-4c6a-b22a-0aa47aed3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_loc_2d = tf.Variable([0., 0.])\n",
    "trainable_scale_2d = tf.Variable([1., 1.])\n",
    "\n",
    "trainable_distr_2d = tfd.MultivariateNormalDiag(\n",
    "    loc=trainable_loc_2d,\n",
    "    scale_diag=trainable_scale_2d\n",
    ")\n",
    "\n",
    "trainable_distr_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18825be1-7dc2-4eea-a479-0811b7a6f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_2d, grad_history_2d, params_history_2d = distr_optimization(\n",
    "    samples_2d,\n",
    "    nll,\n",
    "    trainable_distr_2d,\n",
    "    ['loc', 'scale_diag'],\n",
    "    n_iter=500,\n",
    "    learning_rate=.2,\n",
    "    positive_params=['scale_diag'],\n",
    "    positivity_constraint_method='exp'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'Initial loss: {loss_history_2d[0]}\\n'\n",
    "    f'Final loss: {loss_history_2d[-1]}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572343d-08d9-4d98-be6c-480cc2429088",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(14, 15), sharex=True)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "# Loss.\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history_2d)),\n",
    "    y=loss_history_2d.numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.ylabel('Loss values', fontsize=12)\n",
    "plt.title('Optimization history', fontsize=16)\n",
    "\n",
    "# loc (component 0).\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=params_history_2d[0, :, 0].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=np.ones(params_history_2d.shape[1]) * generating_loc_2d[0],\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.ylabel('loc (comp. 0)\\nparam values', fontsize=12)\n",
    "\n",
    "# loc (component 1).\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=params_history_2d[0, :, 1].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[2]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=np.ones(params_history_2d.shape[1]) * generating_loc_2d[1],\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[2]\n",
    ")\n",
    "\n",
    "plt.sca(axs[2])\n",
    "plt.ylabel('loc (comp. 1)\\nparam values', fontsize=12)\n",
    "\n",
    "# scale_diag (component 0).\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=params_history_2d[1, :, 0].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[3]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=np.ones(params_history_2d.shape[1]) * generating_scale_2d[0],\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[3]\n",
    ")\n",
    "\n",
    "plt.sca(axs[3])\n",
    "plt.ylabel('scale_diag (comp. 0)\\nparam values', fontsize=12)\n",
    "\n",
    "# scale_diag (component 1).\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=params_history_2d[1, :, 1].numpy(),\n",
    "    color=sns.color_palette()[0],\n",
    "    ax=axs[4]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(params_history_2d.shape[1]),\n",
    "    y=np.ones(params_history_2d.shape[1]) * generating_scale_2d[1],\n",
    "    color=sns.color_palette()[1],\n",
    "    ax=axs[4]\n",
    ")\n",
    "\n",
    "plt.sca(axs[4])\n",
    "plt.ylabel('scale_diag (comp. 1)\\nparam values', fontsize=12)\n",
    "plt.xlabel('N iteration', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6105aae-9906-41f0-b512-f87ab5a134f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(14, 10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=params_history_2d[0, :, :].numpy()[:, 0],\n",
    "    y=params_history_2d[0, :, :].numpy()[:, 1],\n",
    "    alpha=0.3,\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=[generating_loc_2d[0]],\n",
    "    y=[generating_loc_2d[1]],\n",
    "    alpha=1.,\n",
    "    ax=axs[0],\n",
    "    color=sns.color_palette()[1]\n",
    ")\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.xlabel('loc_0', fontsize=12)\n",
    "plt.ylabel('loc_1', fontsize=12)\n",
    "plt.title('History of parameters along the optimization', fontsize=16)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=params_history_2d[1, :, :].numpy()[:, 0],\n",
    "    y=params_history_2d[1, :, :].numpy()[:, 1],\n",
    "    alpha=0.3,\n",
    "    ax=axs[1]\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=[generating_scale_2d[0]],\n",
    "    y=[generating_scale_2d[1]],\n",
    "    alpha=1.,\n",
    "    ax=axs[1],\n",
    "    color=sns.color_palette()[1]\n",
    ")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.xlabel('scale_diag_0', fontsize=12)\n",
    "plt.ylabel('scale_diag_1', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
