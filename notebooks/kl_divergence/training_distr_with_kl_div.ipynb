{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df894828-394e-4721-af17-c15de8891538",
   "metadata": {},
   "source": [
    "# Training distributions using the KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda18f3d-c10c-4e2f-a87c-775612b7d623",
   "metadata": {},
   "source": [
    "__Objective:__ train a TFP distribution to approximate a given one minimizing the Kullback-Leibler divergence between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0259d23-0f06-44bd-8b47-951b18d5c23b",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence between distributions $p$ and $q$ is defined as\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "D_\\mathrm{KL} [p || q] &\\equiv& - \\int \\mathrm{d}^dx\\, p(x) \\log \\left( \\frac{q(x)}{p(x)} \\right)\\\\\n",
    "&=& \\mathbb{E}_{x \\sim p} \\left[ \\log(p(x)) - \\log(q(x))\\right]\n",
    "\\end{array}\n",
    "$$\n",
    "and quantifies how much $p$ differs from $q$. Notice however that it can't be regarded as a metric on the space of distributions, as it isn't symmetric.\n",
    "\n",
    "If $p$ is parametrized by a set of parameters $\\theta$, we can minimize $D_\\mathrm{KL} [p || q]$ w.r.t. $\\theta$, finding the optimal parameters minimizing the difference between the two distributions. Because the KL divergence is not symmetric, minimizing $D_\\mathrm{KL} [q || p]$, though perfectly legit, would give a different result - in particular, the $D_\\mathrm{KL}$ tends to give better (lower) scores if the support of the **first** distribution is contained in that of the second one. Therefore,\n",
    "- Minimizing $D_\\mathrm{KL} [p || q]$ (with the trainable distribution as the first argument) will tend to find $p$ with support contained in the support of $q$ (which can lead to an undersetimate of its variance).\n",
    "- Minimizing $D_\\mathrm{KL} [q || p]$ (with the trainable distribution as the second argument) will tend to find $p$ with a support that contains that of $q$ (which in turns can lead to an overestimate of its variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea6c25-05e9-4249-b089-b86209fbcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from keras_utilities import plot_history\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edb38f4-ea02-435d-9c4a-ec9de7497c17",
   "metadata": {},
   "source": [
    "## Target distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8a022-1324-4aaf-8157-bb91ff964c58",
   "metadata": {},
   "source": [
    "Define the target distribution: a 2-dimensional multivariate Gaussian distribution with full covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b54b2d-e5aa-4ebc-bc29-a3e1750248dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the target distribution.\n",
    "q_mu = [0., 0.]\n",
    "\n",
    "# Bijector that , given a general vector, generates the\n",
    "# lower-triangular part of the covariance matrix.\n",
    "lower_triangle_bij = tfb.Chain([\n",
    "    # Applies the given bijector (in this case softplus\n",
    "    # to the diagonal entries a matrix.\n",
    "    tfb.TransformDiagonal(tfb.Softplus()),\n",
    "    # Given a vector, fills in the lower triangle of\n",
    "    # a matrix with the elements of the vector in a\n",
    "    # clockwise spiral way.\n",
    "    tfb.FillTriangular()\n",
    "])\n",
    "\n",
    "# Generate the lower-triangular covariance matrix.\n",
    "q_l = lower_triangle_bij(tf.random.uniform(shape=(3,)))\n",
    "\n",
    "# Create a multivariate normal distribution from the\n",
    "# lower-triangular covariance matrix.\n",
    "q = tfd.MultivariateNormalTriL(loc=q_mu, scale_tril=q_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08ff4e-b1af-41d3-9ecc-e0fa5d90f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = (\n",
    "    (q.parameters['loc'][0] - 3. * q.parameters['scale_tril'][0, 0]).numpy(),\n",
    "    (q.parameters['loc'][0] + 3. * q.parameters['scale_tril'][0, 0]).numpy()\n",
    ")\n",
    "\n",
    "y_min, y_max = (\n",
    "    (q.parameters['loc'][1] - 3. * q.parameters['scale_tril'][1, 1]).numpy(),\n",
    "    (q.parameters['loc'][1] + 3. * q.parameters['scale_tril'][1, 1]).numpy()\n",
    ")\n",
    "\n",
    "x_plot, y_plot = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 1000, dtype=np.float32),\n",
    "    np.linspace(y_min, y_max, 1000, dtype=np.float32),\n",
    ")\n",
    "\n",
    "prob_plot = q.prob(tf.stack(\n",
    "    [x_plot.flatten(), y_plot.flatten()],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.contour(\n",
    "    x_plot,\n",
    "    y_plot,\n",
    "    np.reshape(prob_plot, x_plot.shape),\n",
    "    cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.title('Target distribution', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed645bc-07cd-48d5-9f12-a6b44385c454",
   "metadata": {},
   "source": [
    "## Variational distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34805092-9a9b-4dbb-b2b8-8de436b18379",
   "metadata": {},
   "source": [
    "Define the approximate distribution we'll use to approximate the target one: a multivariate Gaussian with diagonal covariance.\n",
    "\n",
    "**Note:** the target distribution exhibits correlation between the two dimensions (covariance matrix is not diagonal), while the approximate distribution does not (diagonal covariance). This means that the target distribution is **not** in the same family of distributions parametrized by the approximate one, so we won't be able to match it perfectly with the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa6102-06fd-4187-801e-4e0317e1c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tfd.MultivariateNormalDiag(\n",
    "    # Randomly initialized mean vector.\n",
    "    loc=tf.Variable(tf.random.normal(shape=(2,))),\n",
    "    # Randomly initialized diagon entries of the covariance\n",
    "    # matrix (the other entries are assumed to be zero).\n",
    "    scale_diag=tf.Variable(tfb.Exp()(tf.random.uniform(shape=(2,))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249105f3-b177-49ec-957c-ea2029c63833",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min_p, x_max_p = (\n",
    "    (p.parameters['loc'][0] - 3. * p.parameters['scale_diag'][0]).numpy(),\n",
    "    (p.parameters['loc'][0] + 3. * p.parameters['scale_diag'][0]).numpy()\n",
    ")\n",
    "\n",
    "y_min_p, y_max_p = (\n",
    "    (p.parameters['loc'][1] - 3. * p.parameters['scale_diag'][1]).numpy(),\n",
    "    (p.parameters['loc'][1] + 3. * p.parameters['scale_diag'][1]).numpy()\n",
    ")\n",
    "\n",
    "x_plot_p, y_plot_p = np.meshgrid(\n",
    "    np.linspace(x_min_p, x_max_p, 1000, dtype=np.float32),\n",
    "    np.linspace(y_min_p, y_max_p, 1000, dtype=np.float32),\n",
    ")\n",
    "\n",
    "prob_plot_p = p.prob(tf.stack(\n",
    "    [x_plot_p.flatten(), y_plot_p.flatten()],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.contour(\n",
    "    x_plot_p,\n",
    "    y_plot_p,\n",
    "    np.reshape(prob_plot_p, x_plot_p.shape),\n",
    "    cmap='Reds'\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    x_plot,\n",
    "    y_plot,\n",
    "    np.reshape(prob_plot, x_plot.shape),\n",
    "    cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.title('Distributions', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60519a4d-2309-42d1-9afa-975d35483712",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e03ec6-2b3b-4446-a317-1eb5dc8db7e0",
   "metadata": {},
   "source": [
    "The training loop minimizes $D_\\mathrm{KL} [p || q]$ iteratively using a gradient descent-like algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf1f7c-ae82-4212-bb61-e7e02a123dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_and_grads(distributions, loss_f=tfd.kl_divergence, trainable_distr=0):\n",
    "    \"\"\"\n",
    "    Compute the value of the loss function `loss` between\n",
    "    distributions `distr_1` and `distr_2`.\n",
    "    \"\"\"\n",
    "    dist_1, dist_2 = distributions\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_f(dist_1, dist_2)\n",
    "\n",
    "    grad = tape.gradient(loss, distributions[trainable_distr].trainable_variables)\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0122efb-f680-4ba7-b2ba-56185c550cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "history = {\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15272e24-5061-4004-8bce-68d86ca9bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_counter += 1\n",
    "    \n",
    "    loss, grad = loss_and_grads([p, q])\n",
    "\n",
    "    optimizer.apply_gradients(zip(grad, p.trainable_variables))\n",
    "\n",
    "    history['loss'].append(loss.numpy())\n",
    "\n",
    "    if (epoch_counter < 10) or (epoch_counter % 100 == 0):\n",
    "        print(f'Epoch: {epoch_counter} | Loss: {loss}')\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418a907-9dc0-42cf-8e3f-47be9f195ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min_p, x_max_p = (\n",
    "    (p.parameters['loc'][0] - 3. * p.parameters['scale_diag'][0]).numpy(),\n",
    "    (p.parameters['loc'][0] + 3. * p.parameters['scale_diag'][0]).numpy()\n",
    ")\n",
    "\n",
    "y_min_p, y_max_p = (\n",
    "    (p.parameters['loc'][1] - 3. * p.parameters['scale_diag'][1]).numpy(),\n",
    "    (p.parameters['loc'][1] + 3. * p.parameters['scale_diag'][1]).numpy()\n",
    ")\n",
    "\n",
    "x_plot_p, y_plot_p = np.meshgrid(\n",
    "    np.linspace(x_min_p, x_max_p, 1000, dtype=np.float32),\n",
    "    np.linspace(y_min_p, y_max_p, 1000, dtype=np.float32),\n",
    ")\n",
    "\n",
    "prob_plot_p = p.prob(tf.stack(\n",
    "    [x_plot_p.flatten(), y_plot_p.flatten()],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.contour(\n",
    "    x_plot_p,\n",
    "    y_plot_p,\n",
    "    np.reshape(prob_plot_p, x_plot_p.shape),\n",
    "    cmap='Reds'\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    x_plot,\n",
    "    y_plot,\n",
    "    np.reshape(prob_plot, x_plot.shape),\n",
    "    cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.title('Distributions', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735729a-1027-424d-9309-4c1ef9e6c630",
   "metadata": {},
   "source": [
    "### Experiment: swap the terms in the KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd337c-e295-443c-968f-7aba2de64f5b",
   "metadata": {},
   "source": [
    "Let's reinitialize the approximate distribution $p$ and minimize $D_\\mathrm{KL} [q || p]$ this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe76d9-8493-4617-8215-4ec90248d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2 = tfd.MultivariateNormalDiag(\n",
    "    loc=tf.Variable(tf.random.normal(shape=(2,))),\n",
    "    scale_diag=tf.Variable(tfb.Exp()(tf.random.uniform(shape=(2,))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1702d23-fe55-454f-952f-8d8a19125431",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer_2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "history_2 = {\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "epoch_counter_2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe3e3e2-ccf9-4b64-b45e-0080f8a60b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for i in range(epochs):\n",
    "    epoch_counter_2 += 1\n",
    "\n",
    "    # Here p and q are swapped w.r.t. before.\n",
    "    loss, grad = loss_and_grads([q, p_2], trainable_distr=1)\n",
    "\n",
    "    optimizer_2.apply_gradients(zip(grad, p_2.trainable_variables))\n",
    "\n",
    "    history_2['loss'].append(loss.numpy())\n",
    "\n",
    "    if (epoch_counter_2 < 10) or (epoch_counter_2 % 100 == 0):\n",
    "        print(f'Epoch: {epoch_counter_2} | Loss: {loss}')\n",
    "\n",
    "plot_history(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2f6f8-1989-4321-bc81-a2a107614ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min_p_2, x_max_p_2 = (\n",
    "    (p_2.parameters['loc'][0] - 3. * p_2.parameters['scale_diag'][0]).numpy(),\n",
    "    (p_2.parameters['loc'][0] + 3. * p_2.parameters['scale_diag'][0]).numpy()\n",
    ")\n",
    "\n",
    "y_min_p_2, y_max_p_2 = (\n",
    "    (p_2.parameters['loc'][1] - 3. * p_2.parameters['scale_diag'][1]).numpy(),\n",
    "    (p_2.parameters['loc'][1] + 3. * p_2.parameters['scale_diag'][1]).numpy()\n",
    ")\n",
    "\n",
    "x_plot_p_2, y_plot_p_2 = np.meshgrid(\n",
    "    np.linspace(x_min_p_2, x_max_p_2, 1000, dtype=np.float32),\n",
    "    np.linspace(y_min_p_2, y_max_p_2, 1000, dtype=np.float32),\n",
    ")\n",
    "\n",
    "prob_plot_p_2 = p_2.prob(tf.stack(\n",
    "    [x_plot_p_2.flatten(), y_plot_p_2.flatten()],\n",
    "    axis=1\n",
    "))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.contour(\n",
    "    x_plot_p_2,\n",
    "    y_plot_p_2,\n",
    "    np.reshape(prob_plot_p_2, x_plot_p_2.shape),\n",
    "    cmap='Reds'\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    x_plot,\n",
    "    y_plot,\n",
    "    np.reshape(prob_plot, x_plot.shape),\n",
    "    cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.title('Distributions', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576e620-12f9-429c-92a7-df3eb8cd772b",
   "metadata": {},
   "source": [
    "Compare the two optimized distributions: the one obtained by minimizing $D_\\mathrm{KL} [p || q]$ has a smaller variance than the one obtained minimizing $D_\\mathrm{KL} [q || p]$, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd429f33-f361-4a97-9d67-1a0a03679541",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.contour(\n",
    "    x_plot_p,\n",
    "    y_plot_p,\n",
    "    np.reshape(prob_plot_p, x_plot_p.shape),\n",
    "    cmap='Reds'\n",
    ")\n",
    "\n",
    "plt.contour(\n",
    "    x_plot_p_2,\n",
    "    y_plot_p_2,\n",
    "    np.reshape(prob_plot_p_2, x_plot_p_2.shape),\n",
    "    cmap='Blues'\n",
    ")\n",
    "\n",
    "plt.title('Distributions', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
