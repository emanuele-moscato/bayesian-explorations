{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b243e51-f5de-4d02-8210-0850e7d3e240",
   "metadata": {},
   "source": [
    "# Variational autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef28c2-467b-4e4b-981f-ec5d98eb327a",
   "metadata": {},
   "source": [
    "__Objective:__ define a variational autoencoder for images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77a635-1d45-4631-b6a2-b59c44ad7a8b",
   "metadata": {},
   "source": [
    "**Idea:** in an autoencoder, the encoder maps samples to points in latent space. In a variational autoencoder, it maps samples to **multivariate Gaussian distributions** on latent space. This helps reconstructing similar samples from nearby points in latent space, because the decoder now needs to minimize the reconstruction error for all the points sampled from the distribution corresponding to the same input sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2802441d-cd23-42ce-821f-5fc4b4da1aec",
   "metadata": {},
   "source": [
    "### Ingredients\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "the encoder part of the model is modified to output the parameters for a multivariate Gaussian on latent space with diagonal covariance matrix. In practice, given the input sample $x$, the encoder outputs two vectors $\\mu(x), \\sigma(x) \\in \\mathbb{R}^d$, where $d$ is the dimension of latent space, parametrizing a distribution $\\mathcal{N}(\\mu(x), \\Sigma(x))$, where $\\Sigma(x) = \\mathrm{diag}(\\sigma^2_1(x), \\ldots, \\sigma^2_d(x))$.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "latent vectors $z\\in \\mathbb{R}^d$ are obtained by sampling the distributions on latent space, and given a latent vector the decoder produces a realistic sample, as similar as possible to the one correspnding to the Gaussian distribution that generated $z$. The architecture of the decoder indeed remains the same as in regular autoencoders.\n",
    "\n",
    "#### Loss function\n",
    "\n",
    "The loss function to minimize has an additional term w.r.t. the usual MSE or categorical cross-entropy consisting in the KL divergence of the Gaussian distribution on the latent space corresponding to each sample and a (multivariate) standard normal distribution,\n",
    "$$\n",
    "\\mathrm{KL}\\left[ \\mathcal{N}(\\mu(x), \\Sigma(x)) || \\mathcal{N}(0, \\mathbf{1}) \\right]\\,.\n",
    "$$\n",
    "This comes from assuming a multivariate standard normal prior on latent space, a Gaussian likelihood and an approximate variational posterior given by the multivariate Gaussian outputted by the encoder. With the reparametrization trick, the loss function is then given by the KL-divergence of the variational posterior and the true posterior (product of likelihood and prior).\n",
    "\n",
    "The KL divergence above can be computed analytically, so given $\\mu(x)$ and $\\sigma(x)$ it's easy to compute the exact contribution to the total loss:\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\mathrm{KL}\\left[ \\mathcal{N}(\\mu(x), \\Sigma(x)) || \\mathcal{N}(0, \\mathbf{1}) \\right] &\\equiv& -\\int \\mathrm{d}^d z\\, \\mathcal{N}(z | \\mu(x), \\sigma(x))\\,\\log\\left( \\frac{\\mathcal{N}(z | 0, \\mathbf{1})}{\\mathcal{N}(z | \\mu(x), \\sigma(x))} \\right) \\\\\n",
    "&=& -\\frac{1}{2} \\sum_{j=1}^d \\left( 1 + \\log(\\sigma^2_j) - \\mu_j^2 - \\sigma_j^2 \\right)\\,.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In the $\\beta$-VAE variant of the model it's possible to tune the relative weight of the reconstruction and KL terms in the loss functions via a coefficient $\\beta$,\n",
    "$$\n",
    "\\mathcal{L} = \\mathrm{MSE} + \\beta\\,\\mathrm{KL}\\,.\n",
    "$$\n",
    "$\\beta$ is an hyperparameter controlling the balance between the minimization of either term in the loss: if $\\beta$ is too small the KL term will have little effect (latent vectors more spread in latent space, farther away from the origin and with discontinuoous clusters), while if $\\beta$ is too big the KL term will prevail and the model will have a poor recontruction power (essentially the Gaussians will end up fitting the unit ones).\n",
    "\n",
    "#### Reparametrization trick\n",
    "\n",
    "Given an input sample, the prediction has a random component corresponding to the sampling of the Gaussian distribution obtained from the input sample via the encoder. Backpropagation would require to \"differentiate the sampling\" w.r.t. the parameters of the Gaussian distribution, which is not possible: one drawn, a sample is a numerical value and all the information about the distribution from which it was generated is lost. Nonetheless, it's possible use a reparametrization of the Gaussian distribution that allows for explicit differentiation w.r.t. to the $\\mu(x)$ and $\\sigma(x)$ parameters, the **reparametrization trick**.\n",
    "\n",
    "Given the input sample $x$, the encoder outputs the parameters $\\mu(x)$ and $\\sigma(x)$ of the multivariate Gaussian $\\mathcal{N}(\\mu(x), \\sigma(x))$, from which the latent vector $z$ is sampled,\n",
    "$$\n",
    "z \\sim \\mathcal{N}(\\mu(x), \\sigma(x))\\,.\n",
    "$$\n",
    "The reparametrization trick consists in sampling $z$ in the equivalent way\n",
    "$$\n",
    "z = \\mu(x) + \\sigma(x)\\,\\epsilon\\,,\n",
    "$$\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, 1)$. This way the generated values for $z$ are exactly equivalent as before, but the parameters $\\mu$ and $\\sigma$ appear exlicitly and differentiation w.r.t. them is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f9fe2-b44d-4f7f-bcbd-bd177f9fb1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../../modules/')\n",
    "\n",
    "from variational_autoencoders import VariationalEncoder, SampleLayer\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e364aa-6b96-49a9-82cb-2d913e3382d0",
   "metadata": {},
   "source": [
    "## Model definition and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e2063-9916-4dc3-9af3-38d364d4f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoder = VariationalEncoder()\n",
    "sample_layer = SampleLayer()\n",
    "\n",
    "n_samples = 5000\n",
    "\n",
    "random_inputs = tf.random.normal(shape=(n_samples, 32, 32, 1))\n",
    "\n",
    "z_mean, z_log_var, z_samples = variational_encoder(random_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17485c9b-ea57-4bbb-906d-ce7ac94e2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, variational_encoder, decoder):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.variational_encoder = variational_encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name='reconstruction_loss'\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.Mean(name='kl_loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker\n",
    "        ]\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var, z_samples = self.variational_encoder(x)\n",
    "\n",
    "        reconstructed_samples = self.decoder(z_samples)\n",
    "\n",
    "        return z_mean, z_log_var, reconstructed_samples\n",
    "\n",
    "    def train_step(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, reconstructed_samples = self(x)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.binary_crossentropy(\n",
    "                    x,\n",
    "                    reconstructed_samples,\n",
    "                    axis=(1, 2, 3)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(\n",
    "                \n",
    "            ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
