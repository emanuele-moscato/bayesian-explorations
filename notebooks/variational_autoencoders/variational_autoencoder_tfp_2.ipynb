{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95db28c3-5068-4e00-a074-929adc405b16",
   "metadata": {},
   "source": [
    "# Variational autoencoders with Tensorflow Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35b741-8e1f-4e7d-97c9-c60e785ca7f1",
   "metadata": {},
   "source": [
    "__Objective:__ build a VAE using TFP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d70f91-33bf-4d49-8446-46a3ebae8fa8",
   "metadata": {},
   "source": [
    "**Idea:** in variational autoencoders (VAE), an encoder model maps samples $x$ to latent vectors $z$ and a decoder model maps latent vectors $z$ back to samples $x$, just as for regular autoencoders. This time though the model is generative, as assumptions and optimizations are made on the probability distributions of the various objects.\n",
    "\n",
    "Let $x$ denote a sample and $z\\in\\mathbb{R}^d$ be a latent vector ($d$ is the dimension of the latent space). VAE look for a probabilistic relation between $z$ and $x$, i.e. give $x$ we assume that the corresponding $z$ is not deterministic, but rather given by a probability distribution\n",
    "$$\n",
    "p(z|x),\n",
    "$$\n",
    "in which the sample is a variable we condition over.\n",
    "\n",
    "On the other hand, given a latent vector $z$, we also assume that the corresponding sample $x$ is also given by a distribution,\n",
    "$$\n",
    "p(x|z).\n",
    "$$\n",
    "\n",
    "These two distributions are unknown: the model will try to approximate them. In particular, the **encoder** network is reponsible for approximating $p(z|x)$ (as it's the distribution via which latent vectors are encoded, given a sample), while the **decoder** network is reponsible for approximating $p(x|z)$ (as it's the distribution via which samples are reconstructed given a latent vector).\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The encoder network approximates $p(z|x)$ using **variational inference**: we use a **variational distribution**\n",
    "$$\n",
    "q_\\phi(z|x),\n",
    "$$\n",
    "where $\\phi$ is the set of parameters the distibution depends upon, and approximate $p(z|x)$ by minimizing the difference between $q_\\pi(z|x)$ and $p(z|x)$ w.r.t. the parameters.\n",
    "\n",
    "The **Kullback-Leibler** divergence between $q_\\pi(z|x)$ and $p(z|x)$ is taken as a measure of the difference between them,\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z|x) \\right] &\\equiv& - \\int \\mathrm{d}^d z \\, q_\\phi(z|x) \\log \\left( \\frac{p(z|x)}{q_\\phi(z|x)} \\right) \\\\\n",
    "&=& \\mathbb{E}_{z\\sim q_\\phi} \\left[ \\log(q_\\phi(z|x)) - \\log(p(z|x)) \\right] \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Note:** the KL divergence is **not** a symmetric quantity (it's not a metric on the space of distributions), so the order in which distributions are taken changes the result.\n",
    "\n",
    "Because $p(z|x)$ is not known, the above quantity can't be computed as it is. Something else is needed.\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "We assume the decoder network gives the distribution of samples given latent vectors, depending on a set of parameters (the parameters of the network) $\\theta$,\n",
    "$$\n",
    "p_\\theta(x|z)\\,.\n",
    "$$\n",
    "\n",
    "#### Training\n",
    "\n",
    "If we followed the logic of normal autoencoders, the model should be trained to give the best possible reconstruction of samples. Schematically, as in usual autoencoders, given a sample, a corresponding latent vector is produced and then an output sample is reconstructed by the decoder: we want the output sample to be as similar as possible to the one we started with. VAE add sampling as an additional ingredient: given the input sample $x$ we generate the corresponding latent vector $z$ by sampling $q_\\phi(z|x)$ (the true conditional distribution is unaccessible, so we can only sample the variational one!), while once we have $z$, the output sample is generated by sampling $p_\\theta(x|z)$. Optimization then happens by minimizing a \"reconstruction loss\" w.r.t. the parameters involved in the generation of the output samples, i.e. those of both the encoder and the decoder, $\\phi$ and $\\theta$.\n",
    "\n",
    "In fact, VAE follow a slightly different logic that ends up including the above one. Instead of a reconstruction loss, the starting point is the minimization of $D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z|x) \\right]$. Using Bayes' theorem, we can rewrite $p(z|x)$ precisely in terms of the distribution given by the decoder,\n",
    "$$\n",
    "p(z|x) = \\frac{p_\\theta(x|z)\\,p(z)}{p(x)}\\,,\n",
    "$$\n",
    "where $p(z)$ is a prior distribution on latent vectors and $p(x)$ is the evidence, the true generating distribution of the data. Substituting this in the expression for the KL-divergence gives\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z|x) \\right] &=& \\mathbb{E}_{z\\sim\\phi} \\left[ \\log(q_\\phi(z|x)) - \\log(p_\\theta(x|z)) - \\log(p(z)) + \\log(p(x)) \\right] \\\\\n",
    "&=& D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z) \\right] - \\mathbb{E}_{z\\sim\\phi} \\left[ \\log(p_\\theta(x|z)) \\right] + \\log(p(x))\\,,\n",
    "\\end{array}\n",
    "$$\n",
    "where we recognized that the terms with $q_phi(z|x)$ and $p(z)$ reconstruct a KL-divergence and that $p(x)$ doesn't depend on $z$ and therefore the expectation value acts trivially on it.\n",
    "\n",
    "The KL-divergence between any two distributions is always non-negative and in particular is zero if and only if the two distributions are the same, therefore the final expression is always non-negative and we can rearrange it as\n",
    "$$\n",
    "\\log(p(x)) \\geq \\mathbb{E}_{z\\sim\\phi} \\left[ \\log(p_\\theta(x|z)) \\right] - D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z) \\right].\n",
    "$$\n",
    "The RHS is a lower bound for the (log) evidence and is therefore known as **evidence lower bound (ELBO)**. We wan to find values for the parameters that give maximal evidence, threfore our training task translates into the maximization of ELBO w.r.t. $\\phi$ and $\\theta$.\n",
    "\n",
    "This maximization problem can be restated as the minimization of a loss function given by the negative ELBO,\n",
    "$$\n",
    "\\mathcal{L} = - \\mathbb{E}_{z\\sim\\phi} \\left[ \\log(p_\\theta(x|z)) \\right] + D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z) \\right] \\equiv \\mathcal{L}_R + \\mathcal{L}_\\mathrm{KL},\n",
    "$$\n",
    "where\n",
    "- $\\mathcal{L}_R \\equiv - \\mathbb{E}_{z\\sim\\phi} \\left[ \\log(p_\\theta(x|z)) \\right]$ is nothing but the expected negative log likelihood, which in this case is also the usual **reconstruction loss** (the loss term expected by analogy with the usual autoencoders). This is the case because $z$ itself depends on the input samples $x$ because the expectation value is taken with $z$ drawn from $q_\\phi(z|x)$, so effectively this term compares (a function of) the input samples with the output ones.\n",
    "- $\\mathcal{L}_\\mathrm{KL} \\equiv D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z) \\right]$ is a new term specific for VAE and quantifies the difference between the encoding distribution and a prior distribution (to be chosen) on latent space. This acts as a regularization term for the encoding distribution.\n",
    "\n",
    "Both terms are in fact expectation values over the distribution $q_\\phi(z|x)$, a distribution conditioned on $x$, for which we only have a finite number of (training) samples.\n",
    "\n",
    "#### Modeling assumptions\n",
    "\n",
    "We need to make assumptions on the distributions appearing in the loss function, namely $q_\\phi(z|x)$ (encoder), $p_\\theta(x|z)$ (decoder) and $p(z)$ (prior on latent space). The VAE algorithm uses the following assumptions:\n",
    "- $q_\\phi(z|x)$ is a multivariate Gaussian on latent space with diagonal covariance matrix and parameters (mean vector $\\mu(x)\\in\\mathbb{R}^d$ and diagonal elements $\\sigma(x)\\in\\mathbb{R}^d$ of the covariance matrix) parametrized by the encoder network as function of the input samples $x$:\n",
    "$$\n",
    "q_\\phi(z|x) = \\mathcal{N}\\left( z | \\mu(x), \\mathrm{diag}\\left( \\sigma_1^2(x), \\ldots, \\sigma_d^2(x) \\right) \\right).\n",
    "$$\n",
    "- $p_\\theta(x|z)$ is another multivariate Gaussian with diagonal covariance matrix and parameters (mean vector $\\tilde{\\mu}\\in\\mathbb{R}^D$ and diagonal elements $\\tilde{\\sigma}\\in\\mathbb{R}^D$ of the covariance matrix, where $D$ is the dimension of the space of samples) parametrized by the decoder network as functions of the latent vector $z$:\n",
    "$$\n",
    "p_\\theta(x|z) = \\mathcal{N}\\left( x | \\tilde{\\mu}(z), \\mathrm{diag}\\left(  \\tilde{\\sigma}_1^2(z), \\ldots,  \\tilde{\\sigma}_D^2(z) \\right) \\right).\n",
    "$$\n",
    "- $p(z)$ is a multivariate Gaussian with zero mean and unit variance on latent space:\n",
    "$$\n",
    "p(z) = \\mathcal{N}\\left( z | 0, I \\right).\n",
    "$$\n",
    "\n",
    "With these assumption the KL loss term can be computed analytically,\n",
    "$$\n",
    "\\mathcal{L}_\\mathrm{KL} = D_\\mathrm{KL} \\left[ q_\\phi(z|x) || p(z) \\right] = -\\frac{1}{2} \\sum_{j=1}^d \\left[ 1 + \\log(\\sigma_j^2(x)) - \\mu_j^2(x) - \\sigma_j^2(x) \\right].\n",
    "$$\n",
    "Notice that this is a value for a **single input sample** $x$.\n",
    "\n",
    "#### Reparametrization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d7fbae-939f-4fe7-80db-065c9a37ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d029f3-3156-410b-b725-86bbf612a7ed",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8db19c-2e61-46ed-8dfb-a8ffb1e0ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(img):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Normalize pixel values.\n",
    "    img = img.astype('float32') / 255.\n",
    "\n",
    "    # Add padding.\n",
    "    img = np.pad(img, ((0, 0), (2, 2), (2, 2)), constant_values=0.)\n",
    "    \n",
    "    # The images come in grayscale without an explicit\n",
    "    # channels dimensions. Here we add it.\n",
    "    img = np.expand_dims(img, -1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6980b81-b246-4cda-98e0-13065e8b997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we don't really care about the labels in the y arrays.\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = preprocess_images(x_train)\n",
    "x_test = preprocess_images(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5823afe8-81ac-4b9d-9670-c656206eb7b8",
   "metadata": {},
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0dd5b5-f365-48e6-afd8-a2a6f74131f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_dim = 16\n",
    "\n",
    "encoder_tfp = tf.keras.Sequential([\n",
    "    Conv2D(filters=4, kernel_size=(3, 3), strides=2, activation='relu'),\n",
    "    Conv2D(filters=16, kernel_size=(3, 3), strides=2, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(units=2 * latent_space_dim),\n",
    "    tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.MultivariateNormalDiag(\n",
    "            loc=t[..., :latent_space_dim],\n",
    "            scale_diag=t[..., latent_space_dim:]\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "decoder_tfp = tf.keras.Sequential([\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Reshape(target_shape=(8, 8, 1)),\n",
    "    Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=2, output_padding=0, activation='relu'),\n",
    "    Conv2DTranspose(filters=8, kernel_size=(3, 3), strides=2, output_padding=0, activation='relu'),\n",
    "    Conv2D(filters=1, kernel_size=(4, 4), strides=1, padding='valid'),\n",
    "    tfp.layers.DistributionLambda(\n",
    "        lambda t: tfd.Independent(\n",
    "            tfd.MultivariateNormalDiag(loc=t, scale_diag=tf.ones_like(t)),\n",
    "            reinterpreted_batch_ndims=2\n",
    "        )\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d88781-5963-45c8-847e-2f279600e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 6\n",
    "\n",
    "input_samples = x_train[:n_samples, ...]\n",
    "\n",
    "# Intermediate between encoder and decoder: sample the variational distribution\n",
    "# (outputted by the encoder), map the samples to the corresponding reconstruction\n",
    "# distribution (outputted by the decoder) and then sample the latter.\n",
    "reconstructed_samples = decoder_tfp(\n",
    "    encoder_tfp(input_samples).sample()\n",
    ").sample()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=n_samples, figsize=(14, 6))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(n_samples):\n",
    "        axs[i][j].imshow(\n",
    "            input_samples[j, ...] if i == 0 else reconstructed_samples[j, ...].numpy(),\n",
    "            cmap='gray'\n",
    "        )\n",
    "    \n",
    "        axs[i][j].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876473c-e273-4ef6-a1be-ad87a0edefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleWithReparametrizationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, gaussians):        \n",
    "        means = gaussians.mean()\n",
    "        stddevs = gaussians.stddev()\n",
    "\n",
    "        batch_shape = tf.shape(means)[0]\n",
    "        event_shape = tf.shape(means)[1:]\n",
    "        \n",
    "        reparametrized_samples = (\n",
    "            means\n",
    "            + stddevs * tfd.MultivariateNormalDiag(\n",
    "                loc=tf.zeros(shape=event_shape),\n",
    "                scale_diag=tf.ones(shape=event_shape)\n",
    "            ).sample(batch_shape)\n",
    "        )\n",
    "        \n",
    "        return reparametrized_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19c3e6-a82f-4db8-af7e-580263b5c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_layer = SampleWithReparametrizationLayer()\n",
    "\n",
    "sample_layer(encoder_tfp(input_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22b691-523a-465e-a524-d8312b6121d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model = tf.keras.Sequential([\n",
    "    encoder_tfp,\n",
    "    sample_layer,\n",
    "    decoder_tfp\n",
    "])\n",
    "\n",
    "vae_model(input_samples).sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b652a-1a97-4178-a81e-cbc75a1cd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(distr, input_samples, n_intermediate_samples=1):\n",
    "    \"\"\"\n",
    "    In fact, it's the negative log likelihood.\n",
    "    \"\"\"\n",
    "    if n_intermediate_samples == 1:\n",
    "        loss = - distr.log_prob(input_samples)\n",
    "    elif n_intermediate_samples > 1:\n",
    "        # Unfortunately if we sampled the distribution on latent space\n",
    "        # (outputted by the encoder) multiple times, we'd get a higher-rank\n",
    "        # batch size, which the `Reshape` layer in the decoder cannot\n",
    "        # deal with out of the box.\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        raise Exception('The number of intermediate samples should be positive')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def kl_loss(variational_distr):\n",
    "    # KL-divergence between the variational distribution and\n",
    "    # a multivariate standard Normal distribution on the latent\n",
    "    # space.\n",
    "    # Q: how to treat input samples?\n",
    "    loss = tfd.kl_divergence(\n",
    "        # Output of the encoder (given some input samples).\n",
    "        variational_distr,\n",
    "        # Multivariate Gaussian on latent space.\n",
    "        tfd.MultivariateNormalDiag(\n",
    "            loc=tf.zeros(shape=(variational_distr.batch_shape[0], latent_space_dim)),\n",
    "            scale_diag=tf.ones(shape=(variational_distr.batch_shape[0], latent_space_dim))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e773442-69f4-4aad-8d84-c716b0c652e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "epochs_counter = 0\n",
    "training_history = {\n",
    "    'total_loss': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6f3c7-aad5-4932-aa6b-dcebdff833cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "for i in range(epochs):\n",
    "    epochs_counter += 1\n",
    "\n",
    "    # Fetch training batch.\n",
    "    indices = tf.random.shuffle(\n",
    "        tf.range(tf.shape(x_train)[0])\n",
    "    )[:batch_size]\n",
    "    \n",
    "    batch = tf.gather(x_train, indices)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        total_loss = tf.reduce_mean(\n",
    "            reconstruction_loss(vae_model(batch), batch)\n",
    "            + kl_loss(vae_model.layers[0](batch))\n",
    "        )\n",
    "\n",
    "    training_history['total_loss'].append(total_loss)\n",
    "    \n",
    "    gradient = tape.gradient(total_loss, vae_model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradient, vae_model.trainable_variables))\n",
    "\n",
    "    print(f'Epoch: {epochs_counter} | Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029d718-dc6a-4a13-a686-2d6bded15684",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 6\n",
    "\n",
    "input_samples = x_train[:n_samples, ...]\n",
    "\n",
    "# Intermediate between encoder and decoder: sample the variational distribution\n",
    "# (outputted by the encoder), map the samples to the corresponding reconstruction\n",
    "# distribution (outputted by the decoder) and then sample the latter.\n",
    "reconstructed_samples = decoder_tfp(\n",
    "    encoder_tfp(input_samples).sample()\n",
    ").sample()\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=n_samples, figsize=(14, 6))\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(n_samples):\n",
    "        axs[i][j].imshow(\n",
    "            input_samples[j, ...] if i == 0 else reconstructed_samples[j, ...].numpy(),\n",
    "            cmap='gray'\n",
    "        )\n",
    "    \n",
    "        axs[i][j].grid(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
