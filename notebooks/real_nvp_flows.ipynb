{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c0fa4c-fff1-4386-a534-7ed4f9183fa5",
   "metadata": {},
   "source": [
    "# Real NVP (non-volume-preserving) flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2704aaf-2c40-4bbf-be69-47a7c23d4b2b",
   "metadata": {},
   "source": [
    "__Note:__ it looks like `tfb.real_nvp_default_template` doesn't produce trainable variables that either Keras or Tensorflow can track. This is probably related to [this issue](https://github.com/tensorflow/probability/issues/1439). To overcome this, a `Layer` subclass is defined, using Keras `Dense` layers to mimic what `tfb.real_nvp_default_template` - in which case variables are correctly tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d84059c-870d-422a-9dec-6497d99d37e8",
   "metadata": {},
   "source": [
    "__Objective:__ train a real NVP model.\n",
    "\n",
    "Source: [here](https://github.com/tensorchiefs/dl_book/blob/master/chapter_06/nb_ch06_04.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bd4e2-c462-47c7-8edf-853d8f53f637",
   "metadata": {},
   "source": [
    "**Idea:** build an alternative to inverse autoregressive flows that singles out the first $n$ dimensions. Working on $d$-dimensional event space, as with inverse autoregressive flows we start from a \"complicated space\" with points $\\mathbf{z}\\in\\mathbb{R}^d$ distributed according to the probability density function $p_z(\\mathbf{z})$ we'd like to model. We want to find a transformation $\\mathbf{G}$ whose inverse maps points $\\mathbf{z}$ to points $\\mathbf{x}\\in\\mathbb{R}^d$, with the distribution becoming $p_x(\\mathbf{x})$, which we want to be very simple (e.g. a multivariate isotropic Gaussian). In direct form, $\\mathbf{x} = \\mathbf{G}(\\mathbf{z})$. In real NVP flows, $\\mathbf{G}$ is taken to be the identity for the first $n$ components (working one component at a time, on the corresponding one),\n",
    "\n",
    "$$\n",
    "x_i = G_i(\\mathbf{z}) = z_i\\quad \\forall i = 1, \\ldots, n\\,,\n",
    "$$\n",
    "\n",
    "while for the remaining $d - n$ components $\\mathbf{G}$ is taken to be an affine transformation of the corresponding components, with parameters depending **only on the first $n$ components** and given by a neural networks,\n",
    "\n",
    "$$\n",
    "x_i = G_i(\\mathbf{z}) = \\exp\\left( \\alpha_i(z_1, \\ldots, z_n) \\right)\\, z_i + b(z_1, \\ldots, z_n)\\quad \\forall i = n + 1, \\ldots, d\\,.\n",
    "$$\n",
    "\n",
    "Tensorflow Probability provides the following implementation:\n",
    "- `tfp.bijectors.Bijector`: the general class implementing an invertible transformation (a normalizing flow).\n",
    "- `tfp.bijectors.RealNVP`: the bijector implementing the full real NVP flow.\n",
    "\n",
    "As per the first note, `tfb.real_nvp_default_template`, so the network returning the parameters for the affine transformations was written by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc64340-c874-4d73-a648-f3b7d885f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc7ee9-0321-4d5e-8c2f-b689efce352b",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52108d6e-27e1-4b6c-89c6-16f5f24744ce",
   "metadata": {},
   "source": [
    "Generate a complicated distribution of points in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f17bcc8-c0f0-4ec2-aa86-31c97825df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2500\n",
    "\n",
    "x2_samples = tfd.Normal(loc=0., scale=4.).sample(n_samples)\n",
    "\n",
    "x1_samples = tfd.Normal(loc=.25 * tf.square(x2_samples), scale=tf.ones(n_samples, dtype=tf.float32)).sample()\n",
    "\n",
    "samples = tf.stack(\n",
    "    [x1_samples, x2_samples],\n",
    "    axis=1\n",
    ") / 40.\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff7fde-0314-4e25-b8a2-33a89f78596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0],\n",
    "    y=samples[:, 1],\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b314a21-e9d7-4e8f-89bd-3da28c64ff96",
   "metadata": {},
   "source": [
    "## Build the NVP model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533af12f-e6ac-455f-b2ae-69c2f0f6a239",
   "metadata": {},
   "source": [
    "Build the model as a Keras `Model` object (subclass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522d013-09b8-4782-bf6e-6399a63605a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVPLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, full_dim, num_masked):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.full_dim = full_dim\n",
    "        self.num_masked = num_masked\n",
    "        \n",
    "        self.dense_alpha = Dense(\n",
    "            units=full_dim - num_masked,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.dense_b = Dense(\n",
    "            units=full_dim - num_masked,\n",
    "            activation='relu'\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, x, *inputs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        output = (\n",
    "            self.dense_alpha(x),\n",
    "            self.dense_b(x)\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class RealNVPDeepLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, full_dim, num_masked, hidden_layers):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.full_dim = full_dim\n",
    "        self.num_masked = num_masked\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # self.dense_alpha = Dense(\n",
    "        #     units=full_dim - num_masked,\n",
    "        #     activation='relu'\n",
    "        # )\n",
    "        self.alpha_layers = [\n",
    "            Dense(\n",
    "                units=full_dim - num_masked,\n",
    "                activation='relu'\n",
    "            )\n",
    "            for _ in range(hidden_layers[0])\n",
    "        ]\n",
    "        \n",
    "        # self.dense_b = Dense(\n",
    "        #     units=full_dim - num_masked,\n",
    "        #     activation='relu'\n",
    "        # )\n",
    "        self.b_layers = [\n",
    "            Dense(\n",
    "                units=full_dim - num_masked,\n",
    "                activation='relu'\n",
    "            )\n",
    "            for _ in range(hidden_layers[0])\n",
    "        ]\n",
    "        \n",
    "        \n",
    "    def call(self, x, *inputs):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        alpha = x\n",
    "        b = x\n",
    "        \n",
    "        for layer in self.alpha_layers:\n",
    "            alpha = layer(alpha)\n",
    "        \n",
    "        for layer in self.b_layers:\n",
    "            b = layer(b)\n",
    "        \n",
    "        return (alpha, b)\n",
    "\n",
    "\n",
    "class RealNVP(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Subclass of a Keras `Model` object implementing a real\n",
    "    NVP flow.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, output_dim, num_masked, hidden_layers, **kwargs):\n",
    "        \"\"\"\n",
    "        Constructor of the real NVP.\n",
    "        \"\"\"\n",
    "        super().__init__(kwargs)\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.nets = []\n",
    "        \n",
    "        bijectors = []\n",
    "        \n",
    "        # Number of layers.\n",
    "        num_blocks = 5\n",
    "        \n",
    "        # Number of units in the hidden layers of the NN parametrizing\n",
    "        # the affine transformation in the real NVP flow.\n",
    "        h = 32\n",
    "        \n",
    "        use_hidden_layers = not ((hidden_layers is None) or (hidden_layers == 0))\n",
    "        \n",
    "        if use_hidden_layers:\n",
    "            print('Instantiating real NVP model with hidden layers')\n",
    "        else:\n",
    "            print('Instantiating real NVP model with no hidden layer')\n",
    "        \n",
    "        # Each block (layer) is composed of a real NVP flow and a\n",
    "        # permutation, written in this order but then applied in\n",
    "        # reversed order (first the permutation, then the real NVP).\n",
    "        # The resulting first permutation is actually discarded (see\n",
    "        # below).\n",
    "        for i in range(num_blocks):\n",
    "            # Build a function to be used to compute the affine\n",
    "            # parameters in the real NVP (in this case, a NN).\n",
    "            # net = tfb.real_nvp_default_template(\n",
    "            #     hidden_layers=[h, h]  # Number of units in each hidden layer (two heads).\n",
    "            # )\n",
    "            if not use_hidden_layers:\n",
    "                net = RealNVPLayer(full_dim=2, num_masked=1)\n",
    "            else:\n",
    "                net = RealNVPDeepLayer(full_dim=2, num_masked=1, hidden_layers=hidden_layers)\n",
    "            \n",
    "            # Instantiate a real NVP object and append it to\n",
    "            # the list of bijectors.\n",
    "            bijectors.append(\n",
    "                tfb.RealNVP(\n",
    "                    shift_and_log_scale_fn=net,\n",
    "                    # Number of masked dimensions.\n",
    "                    # Note: in 2 dimensions this can only be 1 to get a\n",
    "                    #       nontrivial case.\n",
    "                    num_masked=num_masked\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Instantiate a bijector implementing the permutation\n",
    "            # operation among dimensions, so that singling out the\n",
    "            # first n dimensions in the real NVP doesn't select\n",
    "            # the same ones in each layer (block).\n",
    "            # Note: the argument is the permutation to be used,\n",
    "            #       which in our 2-dimensional case can be only\n",
    "            #       [1, 0] ([0, 1] would be the identity).\n",
    "            bijectors.append(tfb.Permute([1, 0]))\n",
    "            \n",
    "            # Append the neural network function (parametrizing the\n",
    "            # affine parameters) to keep track of it.\n",
    "            self.nets.append(net)\n",
    "            \n",
    "        # Build the full bijector corresponding to the real NVP by\n",
    "        # chaining together the bijectors in the `bijectors` list.\n",
    "        # Notes: \n",
    "        #   * We reverse the list of bijectors so that they are\n",
    "        #     applied in reversed order w.r.t. the one we populated\n",
    "        #     the list with.\n",
    "        #   * Before reversing the list, we leave out the last biijector,\n",
    "        #     which whould be a useless initial permutation.\n",
    "        bijector = tfb.Chain(list(reversed(bijectors[:-1])))\n",
    "        \n",
    "        # Instantiate the flow object: a distribution obtained starting\n",
    "        # from simple source distribution and then applying the full\n",
    "        # bijector obtained above.\n",
    "        self.flow = tfd.TransformedDistribution(\n",
    "            # Source distribution.\n",
    "            distribution=tfd.MultivariateNormalDiag(loc=[0., 0.]),\n",
    "            # Bijector (NF) to apply.\n",
    "            bijector=bijector\n",
    "        )\n",
    "        \n",
    "    def call(self, *inputs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        return self.flow.bijector.forward(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05c9e0-b184-4b60-87ff-0b15f8725018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom layers.\n",
    "print('Test with no hidden layers:')\n",
    "print(\n",
    "    tfb.RealNVP(\n",
    "        num_masked=1,\n",
    "        shift_and_log_scale_fn=RealNVPLayer(full_dim=2, num_masked=1),\n",
    "    ).forward(tf.random.uniform(shape=(5, 2)))\n",
    ")\n",
    "\n",
    "print('\\nTest with hidden layers:')\n",
    "print(\n",
    "    tfb.RealNVP(\n",
    "        num_masked=1,\n",
    "        shift_and_log_scale_fn=RealNVPDeepLayer(full_dim=2, num_masked=1, hidden_layers=[4, 4]),\n",
    "    ).forward(tf.random.uniform(shape=(5, 2)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e906b2-c9ab-4358-b017-c52bd2cf18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RealNVP(output_dim=2, num_masked=1, hidden_layers=None)\n",
    "\n",
    "# Test on some random data.\n",
    "test_model(tf.random.uniform(shape=(5, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ad8ee-093f-4fc7-ad8e-d34914885c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccb48f-1b9a-44f6-99a7-eea01ca3d7e8",
   "metadata": {},
   "source": [
    "**Note:** what the untrained flow does depends on the random initialization of the NN weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9c1a33-3dc9-4896-8a59-81809dd096fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the source distribution and transform the samples via the\n",
    "# untrained flow. This should give nothing sensible!\n",
    "# Note: every time the model is instantiated, the NN weights\n",
    "#       in it are re-initialized and a different transformation\n",
    "#       is obtained.\n",
    "transformed_samples_untrained = test_model(test_model.flow.distribution.sample((2500)))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0].numpy(),\n",
    "    y=samples[:, 1].numpy(),\n",
    "    label='Original samples'\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples_untrained[:, 0].numpy(),\n",
    "    y=transformed_samples_untrained[:, 1].numpy(),\n",
    "    label='Transformed samples (untrained flow)'\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669b17a-349c-41fa-81d5-9c17c01723a7",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa21bc-93dc-4d1b-93ba-33295eb4e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(samples, distr):\n",
    "    \"\"\"\n",
    "    Negative log likelihood of `samples` according to the\n",
    "    distribution `distr`.\n",
    "    \"\"\"\n",
    "    return - tf.reduce_mean(distr.log_prob(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69639b26-1205-47cb-bef6-944c0746f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RealNVP(output_dim=2, num_masked=1, hidden_layers=None)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a8e68-c9b2-4efa-bb06-2a41c81851cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(x):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll(x, model.flow)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62445529-a5e7-4ffe-9d80-7f8585295f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = training_step(samples)\n",
    "        \n",
    "    loss_history.append(loss.numpy())\n",
    "    \n",
    "    if (i < 10) or (i % 100 == 0):\n",
    "        print(f'Epoch: {i} | Loss: {loss_history[-1]}')\n",
    "    \n",
    "loss_history.append(nll(samples, model.flow).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afe1577-7280-4b7c-9b53-0bdf64fd2404",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history)),\n",
    "    y=loss_history\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0].numpy(),\n",
    "    y=samples[:, 1].numpy(),\n",
    "    label='Original samples'\n",
    ")\n",
    "\n",
    "# Sample the source distribution and transform the samples via the\n",
    "# trained flow. This should give something that looks much more\n",
    "# like the original samples, as the flow should have understood\n",
    "# how to map the simple space into the complicated one.\n",
    "transformed_samples = model(model.flow.distribution.sample((2500)))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples[:, 0].numpy(),\n",
    "    y=transformed_samples[:, 1].numpy(),\n",
    "    label='Transformed samples (trained flow)',\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
