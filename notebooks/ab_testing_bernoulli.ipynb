{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0372b681-e618-4e7a-88a5-cf16f7b0d764",
   "metadata": {},
   "source": [
    "# A/B testing with Bernoulli trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1095a65-5693-43a8-a476-fee3689d4d0f",
   "metadata": {},
   "source": [
    "Let's say that we want to try two different versions of an e-commerce website to see which one has a higher chance of seeing the customers convert (place an order). This is the typical example of an A/B test, with a control group that gets served one version of the website and a treatment group that gets served the other. The data consists in binary labels corresponding to customers and indicating whether a customer converted (value = 1) or not (value = 0).\n",
    "\n",
    "Idea: we can model our conversions data as a set of Bernoulli trials. The test will then be about whether the probability in these Bernoulli distribution for the control and treatment group is the same or not. In the classical statistic framework of hypothesis testing, we'd have that the the probability being the same between the two groups is the null hypothesis.\n",
    "\n",
    "Source: this notebook is explicitly \"inspired\" by the [corresponding section of the Bayesian methods for hackers book](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_TFP.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346315dc-0010-42c8-93be-e1a82638a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.fftpack import next_fast_len\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import arviz as az\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22b04-eb55-426b-b431-752a44839e34",
   "metadata": {},
   "source": [
    "## Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b39cf-36ce-475c-8c17-ad973343fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_control = tf.constant(np.load('../data/ab_testing_bernoulli/data_control.npy'))\n",
    "data_treatment = tf.constant(np.load('../data/ab_testing_bernoulli/data_treatment.npy'))\n",
    "\n",
    "data_control.shape, data_treatment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56de6e8-9191-4743-a7f8-d7877990de8b",
   "metadata": {},
   "source": [
    "The maximum likelihood estimate of the probability parameter $p$ in a Bernoulli distribution, given the data, is the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5978eb-c78b-46cc-aa39-dfd2439b9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_est_control = tf.reduce_mean(data_control)\n",
    "p_est_treatment = tf.reduce_mean(data_treatment)\n",
    "p_est_pooled = tf.reduce_mean(tf.concat([data_control, data_treatment], axis=0))\n",
    "\n",
    "print('Maximum likelihood estimate for p for control group:', p_est_control.numpy())\n",
    "print('Maximum likelihood estimate for p for treatment group:', p_est_treatment.numpy())\n",
    "print('Maximum likelihood estimate for p for the pooling of the groups:', p_est_pooled.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321de3-3c05-4509-82d4-f973ac979c07",
   "metadata": {},
   "source": [
    "Let's write a batch of 3 Bernoulli distributions, corresponding to the 3 cases above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627ba13-fb7b-4d28-ba74-c3e9fb264d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoullis = tfd.Bernoulli(probs=[p_est_control, p_est_treatment, p_est_pooled])\n",
    "\n",
    "bernoullis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd991d1-dce9-41cb-a80a-a4f0513c5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoullis.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a4d21-b28e-4514-80ba-a2d6754bebb8",
   "metadata": {},
   "source": [
    "As we increase the number of samples, taking the mean for each distribution should return a better and better approximation of the probabilities we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c7d7b-5285-4dc0-a084-e828dfb0ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.cast(bernoullis.sample(100000), tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da3c7a-914e-4efa-bd5c-1bacf99d111d",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cc908-d20f-46a4-85f7-b554f75f1457",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Not much tuning of the parameters for MCMC has been done, but it should have been!\n",
    "- The method to update the sampling step size is now deprecated and should be updated with its new version.\n",
    "- In general, it's a better idea to sample multiple chains in parallel for the same parameter, to check that they converge to the same distribution (robustness). This hasn't been done here, mostly because starting multiple parallel chains entails reworking the joint log prob so it transparently uses batches of values by broadcasting variables - and getting it right is a bit tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d73426-3667-48f8-bb96-ddf4af03372f",
   "metadata": {},
   "source": [
    "### Inference on probabilities, separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8843-1eb6-4ab6-b88c-c00bb7d495ec",
   "metadata": {},
   "source": [
    "Let's perform Bayesian inference on the parameter $p$ of the two groups separately.\n",
    "\n",
    "There are multiple ways to compute the joint log prob, we explore two here:\n",
    "- We can define a function that given the data and a value of $p$ insantiates the corresponding prior and likelihood and returns the joint log likelihood. This way there's one function that works for all the groups.\n",
    "- We can define a joint probability distribution directly, specifying a (potentially different) prior and likelihood for each group, at the cost of having a different distribution object for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29004bd-b3d3-4554-b95e-5190bb329bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(occurrences, prob):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      occurrences: An array of binary values (0 & 1), representing \n",
    "                   the observed frequency\n",
    "      prob_A: scalar estimate of the probability of a 1 appearing \n",
    "    Returns: \n",
    "      sum of the joint log probabilities from all of the prior and conditional distributions\n",
    "    \"\"\"  \n",
    "    # Prior.\n",
    "    rv_prob= tfd.Uniform(low=0., high=1.)\n",
    "    \n",
    "    # Likelihood.\n",
    "    rv_occurrences = tfd.Bernoulli(probs=prob)\n",
    "\n",
    "    return (\n",
    "        rv_prob.log_prob(prob)\n",
    "        + tf.reduce_sum(rv_occurrences.log_prob(occurrences))\n",
    "    )\n",
    "\n",
    "\n",
    "def trace_stuff(states, previous_kernel_results):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # I couldn't find a way not to make the counter global.\n",
    "    step = next(counter)\n",
    "    \n",
    "    if (step % 100) == 0:\n",
    "        print(f\"Step {step}, state: {states}\")\n",
    "    \n",
    "    return previous_kernel_results\n",
    "\n",
    "\n",
    "def autocov(ary, axis=-1):\n",
    "    \"\"\"Compute autocovariance estimates for every lag for the input array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary : Numpy array\n",
    "        An array containing MCMC samples\n",
    "    Returns\n",
    "    -------\n",
    "    acov: Numpy array same size as the input array\n",
    "    \"\"\"\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= np.conjugate(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n",
    "\n",
    "\n",
    "def autocorr(ary, axis=-1):\n",
    "    \"\"\"Compute autocorrelation using FFT for every lag for the input array.\n",
    "    See https://en.wikipedia.org/wiki/autocorrelation#Efficient_computation\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary : Numpy array\n",
    "        An array containing MCMC samples\n",
    "    Returns\n",
    "    -------\n",
    "    acorr: Numpy array same size as the input array\n",
    "    \"\"\"\n",
    "    corr = autocov(ary, axis=axis)\n",
    "    axis = axis = axis if axis > 0 else len(corr.shape) + axis\n",
    "    norm = tuple(\n",
    "        slice(None, None) if dim != axis else slice(None, 1) for dim, _ in enumerate(corr.shape)\n",
    "    )\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr /= corr[norm]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3ec2a-b0c5-4ade-907d-1dd6027820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_distr_control = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(probs=p * tf.ones_like(data_control)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])\n",
    "\n",
    "joint_distr_treatment = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(probs=p * tf.ones_like(data_treatment)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e7822-4fbb-4993-9be5-80452ac204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the two methods return the same joint log propbability.\n",
    "print(\n",
    "    'Control:',\n",
    "    joint_log_prob(data_control, p_est_control).numpy(),\n",
    "    joint_distr_control.log_prob(p_est_control, data_control).numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Treatment:',\n",
    "    joint_log_prob(data_treatment, p_est_treatment).numpy(),\n",
    "    joint_distr_treatment.log_prob(p_est_treatment, data_treatment).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77c8d9-bbc7-45f3-9ba6-f216ef13ebaa",
   "metadata": {},
   "source": [
    "Note: there might be a small discrepancy between the two methods for computing the joing log probability in the treatment group. This is due to a rounding error introduced when computing the log in the log prob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec7d52-37be-425f-9da4-bdb62d0bea4c",
   "metadata": {},
   "source": [
    "#### Inference on the control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a180106-2c60-4e6f-8252-1503f9d267d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a closure over our joint_log_prob.\n",
    "# The closure makes it so the HMC doesn't try to change the `occurrences` but\n",
    "# instead determines the distributions of other parameters that might generate\n",
    "# the `occurrences` we observed.\n",
    "unnormalized_posterior_log_prob_control = lambda p: joint_distr_control.log_prob(p, data_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f408b7-3fb8-4a5f-8b23-7635e6a65825",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=2\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_control,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        # The step size adaptation prevents stationarity to occur, so the\n",
    "        # number of adaptation steps should be smaller than the number of\n",
    "        # burnin steps so that in the remaining part of the burnin phase\n",
    "        # stationarity can be reached.\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_control_trace = posterior_prob_control[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85199383-0cf5-4356-9f53-236233628996",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ff981-c9b0-4471-9337-3a73c66130e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_posterior_prob_control_trace.numpy().mean(), p_est_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d9e5e-fac7-4988-b643-200e74c6e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301234e-5e37-4dce-87d9-e5e3f45b4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plotting.\n",
    "# Trace.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=tf.range(1, burned_posterior_prob_control_trace.shape[0] + 1, dtype=tf.int32).numpy(),\n",
    "    y=burned_posterior_prob_control_trace.numpy(),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Traceplot\", fontsize=18)\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "plt.ylabel(\"Sample\", fontsize=14)\n",
    "\n",
    "# Autocorrelations.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "x_autocorr = np.arange(1, burned_posterior_prob_control_trace.shape[0])\n",
    "\n",
    "plt.bar(\n",
    "    x_autocorr,\n",
    "    autocorr(burned_posterior_prob_control_trace.numpy())[1:],\n",
    "    width=1,\n",
    "    label=\"$m$\",\n",
    "    edgecolor=sns.color_palette()[0],\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "plt.title(\"Autocorrelation plot of traces for differing $k$ lags\")\n",
    "plt.ylabel(\"Correlation \\nbetween $x_t$ and $x_{t-k}$\")\n",
    "plt.xlabel(\"k (lag)\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Posterior distribution.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45171ddf-0793-45ff-8201-e186d43414f2",
   "metadata": {},
   "source": [
    "#### Inference on the treatment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb28ac-cba8-426a-9b0f-090c175b84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_posterior_log_prob_treatment = lambda p: joint_distr_treatment.log_prob(p, data_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240c88d-fdd3-4bf4-8b93-f6e6a8e6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=2\n",
    "\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.cast(data_treatment, tf.float32))\n",
    "]\n",
    "\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_treatment,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_treatment\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_treatment_trace = posterior_prob_treatment[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2036c8-9475-4791-8c6c-079188e1c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14792db0-a3a4-4845-a8ff-8affbeb38173",
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_posterior_prob_treatment_trace.numpy().mean(), p_est_treatment.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12434fc-d3d4-46bd-9ea6-d709b5135e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96173927-6107-4758-9963-22983d403899",
   "metadata": {},
   "source": [
    "Plot of the posterior sampled distribution for both groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a133028-bd8d-414d-83ea-8694b516a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Control.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_treatment_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (treatment)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_treatment_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean (treatment)')\n",
    "\n",
    "# Treatment.\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (control)',\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='purple',\n",
    "    label='Posterior mean (control)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737071b-9b6b-4558-85b3-1227fb70e5a6",
   "metadata": {},
   "source": [
    "### Inference on probabilities, both grouops together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d28b2-53e2-46d0-9195-bd647b1924b7",
   "metadata": {},
   "source": [
    "Note: this is conceptually the same thing done above, but with chains sampled in parallel. This is because the model assumes no interaction between the distributions, so the sampling of one does not affect the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c119a-3c76-4b0a-975f-20a479ba1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the samples are independent, the combined joint log prob is just the\n",
    "# sum of the log probs for each group.\n",
    "unnormalized_posterior_log_prob_combined = lambda p_control, p_treatment: (\n",
    "    joint_distr_control.log_prob(p_control, data_control)\n",
    "    + joint_distr_treatment.log_prob(p_treatment, data_treatment)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ed705-8ac9-4931-afe4-04b0c42708de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chain's start state.\n",
    "initial_chain_state = [    \n",
    "    tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "    tf.reduce_mean(tf.cast(data_treatment, tf.float32))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026e1e6-2ebd-44a6-9294-5d810ab8bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the target log prob function behaves as expected.\n",
    "# Note: in case of more than one parameter, because the initial state is\n",
    "#       defined as a list, we need to pass it to the function with a `*`\n",
    "#       (that's probably what TFP does internally).\n",
    "unnormalized_posterior_log_prob_combined(*initial_chain_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8a753-6dca-4fa9-88d2-142f1d0ff530",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=3\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid(),   # Maps R to (0, 1).\n",
    "    tfp.bijectors.Sigmoid()    # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_combined,\n",
    "        num_leapfrog_steps=3,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sample from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control_combined,\n",
    "    posterior_prob_treatment_combined\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff\n",
    ")\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_control_combined_trace = posterior_prob_control_combined[burnin:]\n",
    "burned_posterior_prob_treatment_combined_trace = posterior_prob_treatment_combined[burnin:]\n",
    "burned_delta_trace = (burned_posterior_prob_control_combined_trace - burned_posterior_prob_treatment_combined_trace)[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa87ee-8427-4eee-aeb8-a00496c70c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2971c-0dba-499d-99b0-09604838c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Control:',\n",
    "    burned_posterior_prob_control_combined_trace.numpy().mean(),\n",
    "    p_est_control.numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Treatment:',\n",
    "    burned_posterior_prob_treatment_combined_trace.numpy().mean(),\n",
    "    p_est_treatment.numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0342e9-c3e8-4390-b398-c90306698a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e154dc-4287-42c7-97e9-0c2d742ad46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Fluid.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_combined_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (control)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_combined_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean (control)')\n",
    "\n",
    "# Analytics.\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_treatment_combined_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (treatment)',\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_treatment_combined_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='purple',\n",
    "    label='Posterior mean (treatment)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)\n",
    "\n",
    "\n",
    "# Delta.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    burned_delta_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (delta)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=0.,\n",
    "    ymin=0.,\n",
    "    ymax=20.,\n",
    "    color='r',\n",
    "    label='\"Null hypothesis\"')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Delta', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior for delta', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e261c-5f28-4302-807e-70d0af12515c",
   "metadata": {},
   "source": [
    "### Sampling from multiple chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58301f3a-d3c9-4c8e-9788-07c5c48eabe1",
   "metadata": {},
   "source": [
    "Let's try to sample from multiple chains in parallel in order to do diagnostics on the convergence (assess robustness)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed242ad-f427-4d1d-b76f-90a05635ad57",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mini-guide: how to set up multichain sampling with TFP\n",
    "\n",
    "Sources:\n",
    "- `tfp.mcmc.sample_chain` function [documentation](https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc/sample_chain).\n",
    "- [Issue on GitHub](https://github.com/tensorflow/probability/issues/1093) asking the same question.\n",
    "\n",
    "Sampling can be performed in batches (a batch of chains) and the batch shape should be the same for all the parameters, because the `target_log_prob_fn` function (i.e. the unnormalized log prob, i.e. the closure on the function that returns the log prob given value(s) for the parameters to estimate and the data) needs to return values with a shape equal to the batch shape. In practice, we can restrict to having the number of chains as a single dimension, in which case this reduces to saying that the number of chains should be the same for all the parameters on which MCMC is performed.\n",
    "\n",
    "From the documentation we're told that the number of chains is inferred from the shape of the state (i.e. the initial state), but it's also true that the target log prob function must return results with the corresponding shape. Also, the bijectors (in case HMC is used) need to be able to handle the shape of the states. The ingredients are:\n",
    "- States (initial state in particular, as it's the only on under our control).\n",
    "- Target log prob function.\n",
    "- Bijectors.\n",
    "\n",
    "With one chain,\n",
    "- The initial state is a list of tensors with scalar shape (`()`), one for each parameter.\n",
    "- The target log prob function returns a scalar.\n",
    "- The bijectors are put in a list, with one bijector for each parameter.\n",
    "\n",
    "With `n_chains` chains:\n",
    "- The initial state is still a list of tensors, one for each parameter, but this time each tensor must have shape `(n_chains,)`, which is the \"batch shape\".\n",
    "- The target log prob function must return a tensor with the same batch shape, `(n_chains,)`.\n",
    "- The bijectors are put in a list with one bijector for each parameter, exactly as before (bijectors handle the batch shape automatically).\n",
    "\n",
    "A good idea to make sure everything is working as expected is to define the initial shape as a list of tensors with the desired shape, then pass it as input to the target log prob function and see if the result has the same shape as the tensors. There's only **one tricky point** to remember, irrespective of whether we're running a single or multiple chains: if there's more than one parameters on which to perform MCMC, we need to pass the initial state to the target log prob function prepended with a `*`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0444682-31f2-4a32-851e-3b05f35e7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8509ca-482c-4873-b9ef-31de4f0e8f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First possibility: define a joint distribution object that behaves well with\n",
    "# varying parameter shapes.\n",
    "joint_distr_control = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(\n",
    "            probs=tf.expand_dims(p, -1) * tf.ones_like(data_control)\n",
    "        ),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])\n",
    "\n",
    "joint_distr_treatment = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(\n",
    "            probs=tf.expand_dims(p, -1) * tf.ones_like(data_treatment)\n",
    "        ),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ccbba-5e6b-4d7f-b694-5d53e961500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second possibility: use a target log prob function instead of the log prob of\n",
    "# a joint distribution object.\n",
    "# Idea: whatever the shape of the probabilities, interpret all dimensions except from the\n",
    "# right-most as the batch shape, so that the jont log prob function returns a\n",
    "# batch of log probs upon evaluation.\n",
    "def joint_log_prob_combined(p_control, p_treatment, data_control, data_treatment):\n",
    "    prior_p_control = tfd.Uniform(low=0., high=1.)\n",
    "    prior_p_treatment = tfd.Uniform(low=0., high=1.)\n",
    "    \n",
    "    # The total likelihood is the product of the likelihoods, as control and\n",
    "    # test samples are independent.\n",
    "    likelihood_control = tfd.Bernoulli(probs=tf.expand_dims(p_control, -1))\n",
    "    likelihood_treatment = tfd.Bernoulli(probs=tf.expand_dims(p_treatment, -1))\n",
    "    \n",
    "    return (\n",
    "        tf.reduce_sum(prior_p_control.log_prob(p_control), axis=-1)\n",
    "        + tf.reduce_sum(prior_p_treatment.log_prob(p_treatment), axis=-1)\n",
    "        + tf.reduce_sum(likelihood_control.log_prob(data_control), axis=-1)\n",
    "        + tf.reduce_sum(likelihood_treatment.log_prob(data_treatment), axis=-1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1f7cc-48a7-467d-b908-0e574d21b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target log prob function in the two cases. Comment/uncomment to select which\n",
    "# one to use.\n",
    "# Using a joint distribution object.\n",
    "unnormalized_posterior_log_prob_combined = lambda p_control, p_treatment: (\n",
    "    joint_distr_control.log_prob(p_control, data_control)\n",
    "    + joint_distr_treatment.log_prob(p_treatment, data_treatment)\n",
    ")\n",
    "# Defining a function to return the log prob.\n",
    "# unnormalized_posterior_log_prob_combined = (\n",
    "#     lambda p_control, p_treatment: joint_log_prob_combined(p_control, p_treatment, data_control, data_treatment)\n",
    "# )\n",
    "\n",
    "# # Test if the unnormalized posterior log prob behaves as expected with a\n",
    "# possible initial state as the input.\n",
    "state_batch = [\n",
    "    tf.stack([\n",
    "        tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "    ] * n_chains),\n",
    "    tf.stack([\n",
    "        tf.reduce_mean(tf.cast(data_treatment, tf.float32)),\n",
    "    ] * n_chains)\n",
    "]\n",
    "\n",
    "unnormalized_posterior_log_prob_combined(*state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429e06f-d52e-4375-badf-2a7f4fd7b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=2\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.stack([\n",
    "        tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "    ] * n_chains),\n",
    "    tf.stack([\n",
    "        tf.reduce_mean(tf.cast(data_treatment, tf.float32)),\n",
    "    ] * n_chains)\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid(),  # Maps R to (0, 1).\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        # target_log_prob_fn=unnormalized_posterior_log_prob_control,\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_combined,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        # The step size adaptation prevents stationarity to occur, so the\n",
    "        # number of adaptation steps should be smaller than the number of\n",
    "        # burnin steps so that in the remaining part of the burnin phase\n",
    "        # stationarity can be reached.\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control_combined,\n",
    "    posterior_prob_treatment_combined\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps + burnin,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "trace_control_combined_burned = posterior_prob_control_combined[burnin:]\n",
    "trace_treatment_combined_burned = posterior_prob_treatment_combined[burnin:]\n",
    "\n",
    "inference_data = az.convert_to_inference_data({\n",
    "    'p_control': tf.transpose(trace_control_combined_burned),\n",
    "    'p_treatment': tf.transpose(trace_treatment_combined_burned)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7edd65-6125-49dc-a20b-143b8841446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f10603-7228-47e4-b94b-7e8d2814050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f61d9-3c9b-414b-9bc8-d19f7b3e9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(inference_data)\n",
    "\n",
    "az.plot_autocorr(inference_data)\n",
    "\n",
    "az.plot_posterior(inference_data)\n",
    "\n",
    "az.plot_forest(inference_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
