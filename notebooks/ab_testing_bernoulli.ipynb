{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0372b681-e618-4e7a-88a5-cf16f7b0d764",
   "metadata": {},
   "source": [
    "# A/B testing with Bernoulli trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1095a65-5693-43a8-a476-fee3689d4d0f",
   "metadata": {},
   "source": [
    "Let's say that we want to try two different versions of an e-commerce website to see which one has a highers chance of seeing the customers convert (place an order). This is the typical example of an A/B test, with a control group that gets served one version of the website and a treatment group that gets served the other. The data consists in binary labels corresponding to customers and indicating whether a customer converted (value = 1) or not (value = 0).\n",
    "\n",
    "Idea: we can model our conversions data as a set of Bernoulli trials. The test will then be about whether the probability in these Bernoulli distribution for the control and treatment group is the same or not. In the classical statistic framework of hypothesis testing, we'd have that the the probability being the same between the two groups is the null hypothesis.\n",
    "\n",
    "Source: this notebook is explicitly \"inspired\" by the [corresponding section of the Bayesian methods for hackers book](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_TFP.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346315dc-0010-42c8-93be-e1a82638a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "from scipy.fftpack import next_fast_len\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import arviz as az\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea22b04-eb55-426b-b431-752a44839e34",
   "metadata": {},
   "source": [
    "## Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b39cf-36ce-475c-8c17-ad973343fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_control = tf.constant(np.load('../data/ab_testing_bernoulli/data_control.npy'))\n",
    "data_treatment = tf.constant(np.load('../data/ab_testing_bernoulli/data_treatment.npy'))\n",
    "\n",
    "data_control.shape, data_treatment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56de6e8-9191-4743-a7f8-d7877990de8b",
   "metadata": {},
   "source": [
    "The maximum likelihood estimate of the probability parameter $p$ in a Bernoulli distribution, given the data, is the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5978eb-c78b-46cc-aa39-dfd2439b9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_est_control = tf.reduce_mean(data_control)\n",
    "p_est_treatment = tf.reduce_mean(data_treatment)\n",
    "p_est_pooled = tf.reduce_mean(tf.concat([data_control, data_treatment], axis=0))\n",
    "\n",
    "print('Maximum likelihood estimate for p for control group:', p_est_control.numpy())\n",
    "print('Maximum likelihood estimate for p for treatment group:', p_est_treatment.numpy())\n",
    "print('Maximum likelihood estimate for p for the pooling of the groups:', p_est_pooled.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45321de3-3c05-4509-82d4-f973ac979c07",
   "metadata": {},
   "source": [
    "Let's write a batch of 3 Bernoulli distributions, corresponding to the 3 cases above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627ba13-fb7b-4d28-ba74-c3e9fb264d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoullis = tfd.Bernoulli(probs=[p_est_control, p_est_treatment, p_est_pooled])\n",
    "\n",
    "bernoullis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd991d1-dce9-41cb-a80a-a4f0513c5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoullis.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a4d21-b28e-4514-80ba-a2d6754bebb8",
   "metadata": {},
   "source": [
    "As we increase the number of samples, taking the mean for each distribution should return a better and better approximation of the probabilities we started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c7d7b-5285-4dc0-a084-e828dfb0ba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(tf.cast(bernoullis.sample(100000), tf.float32), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da3c7a-914e-4efa-bd5c-1bacf99d111d",
   "metadata": {},
   "source": [
    "## Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cc908-d20f-46a4-85f7-b554f75f1457",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Not much tuning of the parameters for MCMC has been done, but it should have been!\n",
    "- The method to update the sampling step size is now deprecated and should be updated with its new version.\n",
    "- In general, it's a better idea to sample multiple chains in parallel for the same parameter, to check that they converge to the same distribution (robustness). This hasn't been done here, mostly because starting multiple parallel chains entails reworking the joint log prob so it transparently uses batches of values by broadcasting variables - and getting it right is a bit tedious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d73426-3667-48f8-bb96-ddf4af03372f",
   "metadata": {},
   "source": [
    "### Inference on probabilities, separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb8843-1eb6-4ab6-b88c-c00bb7d495ec",
   "metadata": {},
   "source": [
    "Let's perform Bayesian inference on the parameter $p$ of the two groups separately.\n",
    "\n",
    "There are multiple ways to compute the joint log prob, we explore two here:\n",
    "- We can define a function that given the data and a value of $p$ insantiates the corresponding prior and likelihood and returns the joint log likelihood. This way there's one function that works for all the groups.\n",
    "- We can define a joint probability distribution directly, specifying a (potentially different) prior and likelihood for each group, at the cost of having a different distribution object for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29004bd-b3d3-4554-b95e-5190bb329bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(occurrences, prob):\n",
    "    \"\"\"\n",
    "    Joint log probability optimization function.\n",
    "        \n",
    "    Args:\n",
    "      occurrences: An array of binary values (0 & 1), representing \n",
    "                   the observed frequency\n",
    "      prob_A: scalar estimate of the probability of a 1 appearing \n",
    "    Returns: \n",
    "      sum of the joint log probabilities from all of the prior and conditional distributions\n",
    "    \"\"\"  \n",
    "    # Prior.\n",
    "    rv_prob= tfd.Uniform(low=0., high=1.)\n",
    "    \n",
    "    # Likelihood.\n",
    "    rv_occurrences = tfd.Bernoulli(probs=prob)\n",
    "\n",
    "    return (\n",
    "        rv_prob.log_prob(prob)\n",
    "        + tf.reduce_sum(rv_occurrences.log_prob(occurrences))\n",
    "    )\n",
    "\n",
    "\n",
    "def trace_stuff(states, previous_kernel_results):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # I couldn't find a way not to make the counter global.\n",
    "    step = next(counter)\n",
    "    \n",
    "    if (step % 100) == 0:\n",
    "        print(f\"Step {step}, state: {states}\")\n",
    "    \n",
    "    return previous_kernel_results\n",
    "\n",
    "\n",
    "def autocov(ary, axis=-1):\n",
    "    \"\"\"Compute autocovariance estimates for every lag for the input array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary : Numpy array\n",
    "        An array containing MCMC samples\n",
    "    Returns\n",
    "    -------\n",
    "    acov: Numpy array same size as the input array\n",
    "    \"\"\"\n",
    "    axis = axis if axis > 0 else len(ary.shape) + axis\n",
    "    n = ary.shape[axis]\n",
    "    m = next_fast_len(2 * n)\n",
    "\n",
    "    ary = ary - ary.mean(axis, keepdims=True)\n",
    "\n",
    "    # added to silence tuple warning for a submodule\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        ifft_ary = np.fft.rfft(ary, n=m, axis=axis)\n",
    "        ifft_ary *= np.conjugate(ifft_ary)\n",
    "\n",
    "        shape = tuple(\n",
    "            slice(None) if dim_len != axis else slice(0, n) for dim_len, _ in enumerate(ary.shape)\n",
    "        )\n",
    "        cov = np.fft.irfft(ifft_ary, n=m, axis=axis)[shape]\n",
    "        cov /= n\n",
    "\n",
    "    return cov\n",
    "\n",
    "\n",
    "def autocorr(ary, axis=-1):\n",
    "    \"\"\"Compute autocorrelation using FFT for every lag for the input array.\n",
    "    See https://en.wikipedia.org/wiki/autocorrelation#Efficient_computation\n",
    "    Parameters\n",
    "    ----------\n",
    "    ary : Numpy array\n",
    "        An array containing MCMC samples\n",
    "    Returns\n",
    "    -------\n",
    "    acorr: Numpy array same size as the input array\n",
    "    \"\"\"\n",
    "    corr = autocov(ary, axis=axis)\n",
    "    axis = axis = axis if axis > 0 else len(corr.shape) + axis\n",
    "    norm = tuple(\n",
    "        slice(None, None) if dim != axis else slice(None, 1) for dim, _ in enumerate(corr.shape)\n",
    "    )\n",
    "    with np.errstate(invalid=\"ignore\"):\n",
    "        corr /= corr[norm]\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3ec2a-b0c5-4ade-907d-1dd6027820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_distr_control = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(probs=p * tf.ones_like(data_control)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])\n",
    "\n",
    "joint_distr_treatment = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(low=0., high=1.),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(probs=p * tf.ones_like(data_treatment)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e7822-4fbb-4993-9be5-80452ac204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the two methods return the same joint log propbability.\n",
    "print(\n",
    "    'Control:',\n",
    "    joint_log_prob(data_control, p_est_control).numpy(),\n",
    "    joint_distr_control.log_prob(p_est_control, data_control).numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Treatment:',\n",
    "    joint_log_prob(data_treatment, p_est_treatment).numpy(),\n",
    "    joint_distr_treatment.log_prob(p_est_treatment, data_treatment).numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77c8d9-bbc7-45f3-9ba6-f216ef13ebaa",
   "metadata": {},
   "source": [
    "Note: there might be a small discrepancy between the two methods for computing the joing log probability in the treatment group. This is due to a rounding error introduced when computing the log in the log prob."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec7d52-37be-425f-9da4-bdb62d0bea4c",
   "metadata": {},
   "source": [
    "#### Inference on the control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a180106-2c60-4e6f-8252-1503f9d267d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a closure over our joint_log_prob.\n",
    "# The closure makes it so the HMC doesn't try to change the `occurrences` but\n",
    "# instead determines the distributions of other parameters that might generate\n",
    "# the `occurrences` we observed.\n",
    "unnormalized_posterior_log_prob_control = lambda p: joint_distr_control.log_prob(p, data_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f408b7-3fb8-4a5f-8b23-7635e6a65825",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=2\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_control,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        # The step size adaptation prevents stationarity to occur, so the\n",
    "        # number of adaptation steps should be smaller than the number of\n",
    "        # burnin steps so that in the remaining part of the burnin phase\n",
    "        # stationarity can be reached.\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_control_trace = posterior_prob_control[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85199383-0cf5-4356-9f53-236233628996",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ff981-c9b0-4471-9337-3a73c66130e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_posterior_prob_control_trace.numpy().mean(), p_est_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d9e5e-fac7-4988-b643-200e74c6e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    burned_posterior_prob_control_trace.numpy(),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301234e-5e37-4dce-87d9-e5e3f45b4e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom plotting.\n",
    "# Trace.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=tf.range(1, burned_posterior_prob_control_trace.shape[0] + 1, dtype=tf.int32).numpy(),\n",
    "    y=burned_posterior_prob_control_trace.numpy(),\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title(\"Traceplot\", fontsize=18)\n",
    "\n",
    "plt.xlabel(\"Iteration\", fontsize=14)\n",
    "plt.ylabel(\"Sample\", fontsize=14)\n",
    "\n",
    "# Autocorrelations.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "x_autocorr = np.arange(1, burned_posterior_prob_control_trace.shape[0])\n",
    "\n",
    "plt.bar(\n",
    "    x_autocorr,\n",
    "    autocorr(burned_posterior_prob_control_trace.numpy())[1:],\n",
    "    width=1,\n",
    "    label=\"$m$\",\n",
    "    edgecolor=sns.color_palette()[0],\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "plt.title(\"Autocorrelation plot of traces for differing $k$ lags\")\n",
    "plt.ylabel(\"Correlation \\nbetween $x_t$ and $x_{t-k}$\")\n",
    "plt.xlabel(\"k (lag)\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Posterior distribution.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45171ddf-0793-45ff-8201-e186d43414f2",
   "metadata": {},
   "source": [
    "#### Inference on the treatment group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb28ac-cba8-426a-9b0f-090c175b84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_posterior_log_prob_treatment = lambda p: joint_distr_treatment.log_prob(p, data_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240c88d-fdd3-4bf4-8b93-f6e6a8e6b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=2\n",
    "\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.cast(data_treatment, tf.float32))\n",
    "]\n",
    "\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_treatment,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_treatment\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_treatment_trace = posterior_prob_treatment[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2036c8-9475-4791-8c6c-079188e1c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14792db0-a3a4-4845-a8ff-8affbeb38173",
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_posterior_prob_treatment_trace.numpy().mean(), p_est_treatment.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12434fc-d3d4-46bd-9ea6-d709b5135e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    burned_posterior_prob_treatment_trace.numpy(),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96173927-6107-4758-9963-22983d403899",
   "metadata": {},
   "source": [
    "Plot of the posterior sampled distribution for both groups together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a133028-bd8d-414d-83ea-8694b516a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Control.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_treatment_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (treatment)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_treatment_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean (treatment)')\n",
    "\n",
    "# Treatment.\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (control)',\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='purple',\n",
    "    label='Posterior mean (control)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737071b-9b6b-4558-85b3-1227fb70e5a6",
   "metadata": {},
   "source": [
    "### Inference on probabilities, both grouops together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d28b2-53e2-46d0-9195-bd647b1924b7",
   "metadata": {},
   "source": [
    "Note: this is conceptually the same thing done above, but with chains sampled in parallel. This is because the model assumes no interaction between the distributions, so the sampling of one does not affect the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c119a-3c76-4b0a-975f-20a479ba1e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the samples are independent, the combined joint log prob is just the\n",
    "# sum of the log probs for each group.\n",
    "unnormalized_posterior_log_prob_combined = lambda p_control, p_treatment: (\n",
    "    joint_distr_control.log_prob(p_control, data_control)\n",
    "    + joint_distr_treatment.log_prob(p_treatment, data_treatment)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8a753-6dca-4fa9-88d2-142f1d0ff530",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 2000\n",
    "burnin = 500\n",
    "leapfrog_steps=3\n",
    "\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [    \n",
    "    tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "    tf.reduce_mean(tf.cast(data_treatment, tf.float32))\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid(),   # Maps R to (0, 1).\n",
    "    tfp.bijectors.Sigmoid()    # Maps R to (0, 1).\n",
    "]\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_combined,\n",
    "        num_leapfrog_steps=3,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sample from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control_combined,\n",
    "    posterior_prob_treatment_combined\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff\n",
    ")\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_control_combined_trace = posterior_prob_control_combined[burnin:]\n",
    "burned_posterior_prob_treatment_combined_trace = posterior_prob_treatment_combined[burnin:]\n",
    "burned_delta_trace = (burned_posterior_prob_control_combined_trace - burned_posterior_prob_treatment_combined_trace)[burnin:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa87ee-8427-4eee-aeb8-a00496c70c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2971c-0dba-499d-99b0-09604838c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Control:',\n",
    "    burned_posterior_prob_control_combined_trace.numpy().mean(),\n",
    "    p_est_control.numpy()\n",
    ")\n",
    "\n",
    "print(\n",
    "    'Treatment:',\n",
    "    burned_posterior_prob_treatment_combined_trace.numpy().mean(),\n",
    "    p_est_treatment.numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0342e9-c3e8-4390-b398-c90306698a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting using ArviZ.\n",
    "az.plot_trace(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    divergences='bottom',\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_autocorr(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    figsize=(16, 6)\n",
    ")\n",
    "\n",
    "az.plot_posterior(\n",
    "    np.array([\n",
    "        burned_posterior_prob_control_combined_trace.numpy(),\n",
    "        burned_posterior_prob_treatment_combined_trace.numpy()\n",
    "    ]),\n",
    "    kind='hist',\n",
    "    figsize=(16, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e154dc-4287-42c7-97e9-0c2d742ad46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# Fluid.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_control_combined_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (control)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_control_combined_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='r',\n",
    "    label='Posterior mean (control)')\n",
    "\n",
    "# Analytics.\n",
    "sns.histplot(\n",
    "    burned_posterior_prob_treatment_combined_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (treatment)',\n",
    "    color='orange'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=burned_posterior_prob_treatment_combined_trace.numpy().mean(),\n",
    "    ymin=0.,\n",
    "    ymax=30.,\n",
    "    color='purple',\n",
    "    label='Posterior mean (treatment)')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('p (sampled)', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior', fontsize=14)\n",
    "\n",
    "\n",
    "# Delta.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    burned_delta_trace,\n",
    "    bins=40,\n",
    "    stat='density',\n",
    "    label='Posterior samples (delta)'\n",
    ")\n",
    "\n",
    "plt.vlines(\n",
    "    x=0.,\n",
    "    ymin=0.,\n",
    "    ymax=20.,\n",
    "    color='r',\n",
    "    label='\"Null hypothesis\"')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Delta', fontsize=12)\n",
    "plt.title('Distribution of samples from the posterior for delta', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e261c-5f28-4302-807e-70d0af12515c",
   "metadata": {},
   "source": [
    "### Sampling from multiple chains\n",
    "\n",
    "### [WIP] (still not working!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58301f3a-d3c9-4c8e-9788-07c5c48eabe1",
   "metadata": {},
   "source": [
    "Let's try to sample from multiple chains in parallel in order to do diagnostics on the convergence (assess robustness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee3eeb-be09-452f-ad90-a2de7590a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chains = 4\n",
    "\n",
    "joint_distr_control = tfd.JointDistributionSequential([\n",
    "    tfd.Independent(\n",
    "        tfd.Uniform(\n",
    "            low=[0.] * n_chains,\n",
    "            high=[1.]  # Automatically broadcast.\n",
    "        ),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    ),\n",
    "    lambda p: tfd.Independent(\n",
    "        tfd.Bernoulli(\n",
    "            probs=tf.expand_dims(p, axis=1) * tf.ones_like(data_control)\n",
    "        ),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])\n",
    "\n",
    "unnormalized_posterior_log_prob_control = lambda p: joint_distr_control.log_prob(p, data_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef641f7-eb4b-41b2-a255-906df7cf2017",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_distr_control.sample_distributions()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5d491-d86e-4078-8dd3-1a202d644118",
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized_posterior_log_prob_control(initial_chain_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429e06f-d52e-4375-badf-2a7f4fd7b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 10\n",
    "burnin = 1\n",
    "leapfrog_steps=2\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.cast(data_control, tf.float32)),\n",
    "] * n_chains\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Sigmoid()   # Maps R to (0, 1).\n",
    "] * n_chains\n",
    "\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob_control,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=step_size,\n",
    "        # The step size adaptation prevents stationarity to occur, so the\n",
    "        # number of adaptation steps should be smaller than the number of\n",
    "        # burnin steps so that in the remaining part of the burnin phase\n",
    "        # stationarity can be reached.\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sampling from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    posterior_prob_control\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff)\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "burned_posterior_prob_control_trace = posterior_prob_control[burnin:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
