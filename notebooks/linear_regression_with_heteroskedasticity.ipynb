{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96286264-d0e5-488c-9a2e-8a94dca5fe62",
   "metadata": {},
   "source": [
    "# Linear regression with heteroskedasticity with neural networks and probabilistic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0304223-0646-4693-82ba-d4e643385d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.losses import mse\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14813ac-ec2b-469a-a4d6-ab358daa4d52",
   "metadata": {},
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536cc7b-92b5-4b9d-9fc7-8918d3115e49",
   "metadata": {},
   "source": [
    "Generate synthetic data that is normally distributed around a straight line. We'll make this more interesting by using a joint distribution where everything is randomly sampled at the same time: the parameters of the distribution of the data (slope, intercept and the variance around the line) and the data itself - from one single joint distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6286e2-b1a2-4be0-ad15-f9265abec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 180\n",
    "\n",
    "x_data = tfd.Uniform(low=-10.5, high=23.).sample(n_points)\n",
    "x_data = tf.reshape(x_data, (x_data.shape[0], 1))\n",
    "\n",
    "joint_distr = tfd.JointDistributionSequential([\n",
    "    tfd.Normal(loc=2.5, scale=3.),  # m\n",
    "    tfd.Uniform(low=-5., high=12.),  # q\n",
    "    tfd.TransformedDistribution(\n",
    "        tfd.HalfNormal(scale=.5),\n",
    "        tfp.bijectors.Shift(shift=10.)),  # sigma\n",
    "    # Note 1: values in the sequence have to be passed to the lambda function\n",
    "    # in the reverse order w.r.t. the one in which they appear.\n",
    "    # Note 2: Independent is used so the final distribution has event_shape\n",
    "    # (and not batch_shape) equal to the number of points, so that each time\n",
    "    # we sample the distribution we get an entire dataset.\n",
    "    lambda sigma, q, m: tfd.Independent(\n",
    "        tfd.Normal(loc=x_data * m + q, scale=tf.abs(x_data) * sigma + 0.1),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "])\n",
    "\n",
    "# Sample the joint distribution.\n",
    "distr, samples = joint_distr.sample_distributions()\n",
    "\n",
    "m_sampled, q_sampled, sigma_sampled, y_data = samples\n",
    "\n",
    "y_data = tf.reshape(y_data, (y_data.shape[0], 1))\n",
    "\n",
    "\n",
    "# Plot samples.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeca894-889e-485a-9f3b-4a18e112a36f",
   "metadata": {},
   "source": [
    "## Fit a linear regression to the data using a neural network (with one linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c421f-06da-429a-b4e9-b2f24851f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(input_shape=(1,), units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bccdc-5fb3-4004-afcd-1b8682724362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091cc4f-082d-4d30-859a-aaa9381a79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    alpha=0.5,\n",
    "    label='Data')\n",
    "\n",
    "x_plot = tf.linspace(\n",
    "    tf.reduce_min(x_data),\n",
    "    tf.reduce_max(x_data),\n",
    "    100)\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    model(tf.reshape(x_plot, (x_plot.shape[0], 1))).numpy(),\n",
    "    label='Fit (before training!)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add7f3fd-3e24-4144-82d0-cd47cdd632ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.0005),\n",
    "    loss=mse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1582516-8047-4341-8182-eec587de6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    epochs=1000,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d56be-8093-45e7-ac3b-a0a185617830",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.plot(\n",
    "    range(len(history.history['loss'])),\n",
    "    history.history['loss'],\n",
    "    label='Fit (before training!)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss function history along epochs', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (mean square error)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3b16c-7d34-4a4d-b7e1-b525a7db4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    alpha=0.5,\n",
    "    label='Data')\n",
    "\n",
    "x_plot = tf.linspace(\n",
    "    tf.reduce_min(x_data),\n",
    "    tf.reduce_max(x_data),\n",
    "    100)\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    model(tf.reshape(x_plot, (x_plot.shape[0], 1))).numpy(),\n",
    "    label='Fit (after training)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34f89a-c9f0-4877-b877-5ebfb2e5ce1c",
   "metadata": {},
   "source": [
    "Compare with a linear regression from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380de8d-c3fd-455e-8b1a-ab0117f7c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True)\n",
    "\n",
    "lr.fit(\n",
    "    x_data.numpy().reshape(-1, 1),\n",
    "    y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efae99e-d937-43a0-83ea-16a19b5753a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(index=0).weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fde074-13b2-4cec-b787-008fc1d9b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c089a88-172c-4348-9d3e-2618d8e89efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    alpha=0.5,\n",
    "    label='Data')\n",
    "\n",
    "x_plot = tf.linspace(\n",
    "    tf.reduce_min(x_data),\n",
    "    tf.reduce_max(x_data),\n",
    "    100)\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    model(tf.reshape(x_plot, (x_plot.shape[0], 1))).numpy(),\n",
    "    label='Fit (neural network)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    lr.predict(tf.reshape(x_plot, (x_plot.shape[0], 1)).numpy()),\n",
    "    label='Fit (linear regression)',\n",
    "    color='green'\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499fbb5-fd46-47bb-aaba-975a5d413776",
   "metadata": {},
   "source": [
    "## Fit with a neural network with a probabilistic layer at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65cf2ee-380f-4785-a345-b2a1e5754b62",
   "metadata": {},
   "source": [
    "Simple fit of sigma for the output Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152f14b1-6865-4c54-a31e-9bccb556d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_model = Sequential([\n",
    "    Dense(input_shape=(1, ), units=2),\n",
    "    tfpl.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :1], scale=tf.math.softplus(t[..., 1:])),\n",
    "            reinterpreted_batch_ndims=1),\n",
    "        convert_to_tensor_fn=tfd.Distribution.sample\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553149b-9c99-49f1-936c-c01e4b501b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a batch of distributions (one for each\n",
    "# input datapoint).\n",
    "probabilistic_model(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741aa373-de5c-423e-a44b-c8f559cf2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This generates a tensor given by the chosen `convert_to_tensor_fn` (in\n",
    "# this case we sample from the output batch of distributions).\n",
    "probabilistic_model.predict(x_data)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d0dba-6ef3-4d55-9166-371219537b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898012c6-c825-436a-aea2-5a50c92e2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure everything works as expected.\n",
    "x_test = tf.reshape(x_data, (x_data.shape[0], 1))[:2]\n",
    "\n",
    "# Target values.\n",
    "y_test = y_data[:2]  # tf.reshape(y_data, (y_data.shape[0], 1))[:1]\n",
    "\n",
    "# Output of the model evaluated on input data (a batch of\n",
    "# distributions).\n",
    "y_pred = probabilistic_model(x_test)\n",
    "\n",
    "nll(\n",
    "    y_test,\n",
    "    probabilistic_model(x_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff90817-6fd7-423d-92f8-76077401139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_model.compile(\n",
    "    optimizer=RMSprop(learning_rate=0.005),\n",
    "    loss=nll\n",
    ")\n",
    "\n",
    "history = probabilistic_model.fit(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    epochs=500,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65654440-2df6-4025-98d9-da0edce4c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.plot(\n",
    "    range(len(history.history['loss'])),\n",
    "    history.history['loss'],\n",
    "    label='Fit (before training!)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss function history along epochs', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (mean square error)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ccbe7-e090-4296-b5b5-4c0fc117afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    alpha=0.5,\n",
    "    label='Data')\n",
    "\n",
    "x_plot = tf.linspace(\n",
    "    tf.reduce_min(x_data),\n",
    "    tf.reduce_max(x_data),\n",
    "    100)\n",
    "x_plot = tf.reshape(x_plot, (x_plot.shape[0], 1))\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    model(x_plot).numpy(),\n",
    "    label='Fit (neural network)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    probabilistic_model(x_plot).mean(),\n",
    "    label='Fit (probabilistic neural network)',\n",
    "    color='green'\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x_data.numpy(),\n",
    "    probabilistic_model(x_data).sample(),\n",
    "    label='Synthetic samples from probabilistic neural network',\n",
    "    color='green',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f18aab-0d51-4f23-8850-b00fbe9464b8",
   "metadata": {},
   "source": [
    "More complicated fit of sigma.\n",
    "\n",
    "**Note:** if we don't introduce any nonlinear activation function, the fit of the mean and standard deviation of the output batch of TFP distributions will unavoidably be linear. In order to have a more complicated model of heteroskedasticity we need to introduce some nonlinearity. Moreover, not all the nonlinear activation functions work as well: withoud altering the structure of the network, `relu` gives a good fit, while `tanh` and `sigmoid` don't. Notice however that in this case the data was synthetic and we knew in advance what we were looking for (standard deviations srhinking and then increasing again as $x$ increases), so there's a bias in what we deem \"good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92453ec8-351e-47a1-98fd-1ea1dafde302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a more complicated model.\n",
    "probabilistic_model_sigma = Sequential([\n",
    "    Dense(input_shape=(1, ), units=4, activation='relu'),\n",
    "    Dense(2),\n",
    "    tfpl.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfd.Independent(\n",
    "            tfd.Normal(loc=t[..., :1], scale=tf.math.softplus(t[..., 1:])),\n",
    "            reinterpreted_batch_ndims=1\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "probabilistic_model_sigma.compile(\n",
    "    optimizer=RMSprop(learning_rate=0.005),\n",
    "    loss=nll\n",
    ")\n",
    "\n",
    "# Train model.\n",
    "history = probabilistic_model_sigma.fit(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    epochs=500,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Plot loss function along the epochs.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.plot(\n",
    "    range(len(history.history['loss'])),\n",
    "    history.history['loss'],\n",
    "    label='Fit (before training!)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss function history along epochs', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (mean square error)')\n",
    "\n",
    "# Plot fit and samples from the output layer.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    alpha=0.5,\n",
    "    label='Data')\n",
    "\n",
    "x_plot = tf.linspace(\n",
    "    tf.reduce_min(x_data),\n",
    "    tf.reduce_max(x_data),\n",
    "    100)\n",
    "x_plot = tf.reshape(x_plot, (x_plot.shape[0], 1))\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    model(x_plot).numpy(),\n",
    "    label='Fit (neural network)',\n",
    "    color='r'\n",
    ")\n",
    "\n",
    "y_plot = probabilistic_model_sigma(x_plot).mean()\n",
    "y_high = y_plot + 2. * probabilistic_model_sigma(x_plot).parameters['distribution'].scale\n",
    "y_low = y_plot - 2. * probabilistic_model_sigma(x_plot).parameters['distribution'].scale\n",
    "\n",
    "plt.fill_between(\n",
    "    x=x_plot.numpy().flatten(),\n",
    "    y1=y_low.numpy().flatten(),\n",
    "    y2=y_high.numpy().flatten(),\n",
    "    color='green',\n",
    "    alpha=0.2,\n",
    "    label='mean $\\pm 2\\sigma$'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x_plot.numpy(),\n",
    "    y_plot,\n",
    "    label='Fit (probabilistic neural network)',\n",
    "    color='green',\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x_data.numpy(),\n",
    "    probabilistic_model_sigma(x_data).sample(),\n",
    "    label='Synthetic samples from probabilistic neural network',\n",
    "    color='green',\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
