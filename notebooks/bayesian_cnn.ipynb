{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e531aa68-5e7b-4f7a-8da5-f4e9f4d0eeb1",
   "metadata": {
    "id": "e531aa68-5e7b-4f7a-8da5-f4e9f4d0eeb1"
   },
   "source": [
    "# Bayesian CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bcf3e9-8ac7-472e-9684-34090a611777",
   "metadata": {
    "id": "84bcf3e9-8ac7-472e-9684-34090a611777"
   },
   "source": [
    "__Objective:__ experiment with a Bayesian CNN to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1CegNuAfAbN",
   "metadata": {
    "id": "g1CegNuAfAbN"
   },
   "outputs": [],
   "source": [
    "# Execute on Colab.\n",
    "# !pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe58b9b-b1d6-44e2-869f-69d11def6f2a",
   "metadata": {
    "id": "1fe58b9b-b1d6-44e2-869f-69d11def6f2a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import keras_cv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5a2ff-3591-45b7-8ee4-0e9eb6a5a1ee",
   "metadata": {
    "id": "0fb5a2ff-3591-45b7-8ee4-0e9eb6a5a1ee"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcac5dc-90aa-4e52-a149-a663daf9c99c",
   "metadata": {
    "id": "edcac5dc-90aa-4e52-a149-a663daf9c99c"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Create a validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.4)\n",
    "\n",
    "print(\n",
    "    'Training:', x_train.shape, y_train.shape,\n",
    "    '\\nValidation:', x_val.shape, y_val.shape,\n",
    "    '\\nTest:', x_test.shape, y_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32aa4b-a2ad-49c1-87d7-1ae15f91300a",
   "metadata": {
    "id": "da32aa4b-a2ad-49c1-87d7-1ae15f91300a"
   },
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'airplane',\n",
    "    1: 'automobile',\n",
    "    2: 'bird',\n",
    "    3: 'cat',\n",
    "    4: 'deer',\n",
    "    5: 'dog',\n",
    "    6: 'frog',\n",
    "    7: 'horse',\n",
    "    8: 'ship',\n",
    "    9: 'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7df40b-3e8e-42a2-9316-abef3405ad72",
   "metadata": {
    "id": "4f7df40b-3e8e-42a2-9316-abef3405ad72"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=6, figsize=(14, 6))\n",
    "\n",
    "random_indices = tf.random.shuffle(tf.range(0, x_train.shape[0]))[:6]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.imshow(x_train[random_indices[i], ...], cmap='gray')\n",
    "\n",
    "    plt.sca(ax)\n",
    "    plt.title(f'Class: {class_labels[y_train[random_indices[i]][0]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1Pwupef1h7l",
   "metadata": {
    "id": "d1Pwupef1h7l"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3b0c1-bf12-497b-b4a5-54cfe010dbbe",
   "metadata": {
    "id": "3bf3b0c1-bf12-497b-b4a5-54cfe010dbbe"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, class_to_eliminate=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Turn images to grayscale.\n",
    "    # Shape transformation: (batch_shape, 32, 32, 3) -> (batch_shape, 32, 32, 1)\n",
    "    # (a single channel).\n",
    "    x_preprocessed = keras_cv.layers.Grayscale(output_channels=1)(x)\n",
    "\n",
    "    # Convert data to tensors and normalize pixel values.\n",
    "    x_preprocessed = tf.constant(x_preprocessed, dtype=tf.float32) / 255.\n",
    "\n",
    "    # Change target tensor shape: (batch_shape, 1) -> (batch_shape,).\n",
    "    y_preprocessed = y[:, 0]\n",
    "\n",
    "    # Eliminate class if required.\n",
    "    if class_to_eliminate is not None:\n",
    "        x_preprocessed = x_preprocessed[y_preprocessed != class_to_eliminate]\n",
    "        y_preprocessed = y_preprocessed[y_preprocessed != class_to_eliminate]\n",
    "\n",
    "    # One-hot encode the targets.\n",
    "    y_preprocessed = tf.one_hot(\n",
    "        y_preprocessed,\n",
    "        depth=10 if class_to_eliminate is None else 9\n",
    "    )\n",
    "\n",
    "    return x_preprocessed, y_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f70b9-69bd-49e7-a95b-8b83a74e38f5",
   "metadata": {
    "id": "843f70b9-69bd-49e7-a95b-8b83a74e38f5"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data(x_train, y_train, class_to_eliminate=7)\n",
    "x_val, y_val = preprocess_data(x_val, y_val, class_to_eliminate=7)\n",
    "x_test, y_test = preprocess_data(x_test, y_test)\n",
    "\n",
    "print(\n",
    "    'Training:', x_train.shape, y_train.shape,\n",
    "    '\\nValidation:', x_val.shape, y_val.shape,\n",
    "    '\\nTest:', x_test.shape, y_test.shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86496ec-1fce-4b3f-99e8-b3c724d8b761",
   "metadata": {
    "id": "a86496ec-1fce-4b3f-99e8-b3c724d8b761"
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10642468-cd79-4160-b0eb-e91852c18744",
   "metadata": {
    "id": "10642468-cd79-4160-b0eb-e91852c18744"
   },
   "source": [
    "Build a Bayesian CNN with `Convolution2DFlipout` layers. By default, only the kernel weights are treated in a Bayesian way: we could force the same for the bias terms but that would further increase the number of parameters (which is already doubled w.r.t. the non-Bayesian counterpart, as now for each of the original weights there are the $\\mu$ and $\\sigma$ parameters of its approximate posterior (variational distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FTyfImcZtWoj",
   "metadata": {
    "id": "FTyfImcZtWoj"
   },
   "source": [
    "Observations:\n",
    "- Adding batch normalization layers helps avoiding exploding gradients. These are usually put after the activation function following convolutional and dense layers.\n",
    "- Once the minimum of the loss has been reached (given a value for the learning rate), increasing the batch size can help \"squeeze\" some information by computing more exact gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7CJtEGWdWT",
   "metadata": {
    "id": "cf7CJtEGWdWT"
   },
   "source": [
    "### Test on a non-Bayesian CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6jWO5wPUWko7",
   "metadata": {
    "id": "6jWO5wPUWko7"
   },
   "outputs": [],
   "source": [
    "class ClassicCNN(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    CNN network implemented as a Keras `Layer` subclass. See\n",
    "    the `BayesianCNN` for the architecture: this is its\n",
    "    non-bayesian counterpart.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the internal layers.\n",
    "        self.conv_block_1 = [\n",
    "            tf.keras.layers.Convolution2D(\n",
    "                filters=8,\n",
    "                kernel_size=(3, 3),\n",
    "                padding='same',\n",
    "                activation='relu'\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        ]\n",
    "\n",
    "        self.conv_block_2 = [\n",
    "            tf.keras.layers.Convolution2D(\n",
    "                filters=16,\n",
    "                kernel_size=(3, 3),\n",
    "                padding='same',\n",
    "                activation='relu'\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        ]\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_block = [\n",
    "            tf.keras.layers.Dense(units=100, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(units=100, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "\n",
    "        self.output_layer = tf.keras.layers.Dense(units=9, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        for conv_layer in self.conv_block_1:\n",
    "            x = conv_layer(x)\n",
    "\n",
    "        for conv_layer in self.conv_block_2:\n",
    "            x = conv_layer(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        for dense_layer in self.dense_block:\n",
    "            x = dense_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define callback to save/reload the model automatically every time\n",
    "# training ends/starts.\n",
    "backup_callback = tf.keras.callbacks.BackupAndRestore(\n",
    "    backup_dir='./models/'\n",
    ")\n",
    "\n",
    "# Input has shape (32, 32, 3) for RGB images and (32, 32, 1)\n",
    "# for grayscale ones.\n",
    "inputs = tf.keras.layers.Input(shape=(32, 32, 1,))\n",
    "outputs = ClassicCNN()(inputs)\n",
    "\n",
    "cnn_model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "cnn_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "cnn_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=cnn_optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XNIctFvIIP-P",
   "metadata": {
    "id": "XNIctFvIIP-P"
   },
   "outputs": [],
   "source": [
    "K.get_value(cnn_model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HVDFoSkcIRo2",
   "metadata": {
    "id": "HVDFoSkcIRo2"
   },
   "outputs": [],
   "source": [
    "K.set_value(cnn_model.optimizer.lr, 1e-6)\n",
    "\n",
    "print('New learning rate:', K.get_value(cnn_model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t5c3zLdbX9wN",
   "metadata": {
    "id": "t5c3zLdbX9wN"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=27007,\n",
    "    callbacks=[backup_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j-WZ75eYZj8q",
   "metadata": {
    "id": "j-WZ75eYZj8q"
   },
   "outputs": [],
   "source": [
    "# Loss history\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(cnn_history.history['loss'])),\n",
    "    y=cnn_history.history['loss'],\n",
    "    label='Total',\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_awtsjhoWhi5",
   "metadata": {
    "id": "_awtsjhoWhi5"
   },
   "source": [
    "### Test on a Bayesian CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LtuTZDQ6uJro",
   "metadata": {
    "id": "LtuTZDQ6uJro"
   },
   "source": [
    "Observations:\n",
    "- The loss values in the Bayesian case around an order of magnitude bigger than in the non-Bayesian one. I think this is expected as VI adds the KL terms to the loss (one for each variational distribution).\n",
    "- Training is more difficult, in that the NN seems to plateau on worse performance w.r.t. to the non-Bayesian case with the equivalent architecture. My impression is that there's a lot of noise in the training process (see points below).\n",
    "- In the Bayesian CNN case, the loss tends to \"bounce back up\" at some point. This could be given by vanishing/exploding gradients.\n",
    "- Batch normalization - while probably a good idea by analogy with the non-Bayesian case - doesn't fully solve the above problem.\n",
    "- Increasing the batch size seems to be have a bigger effect on the above problem. Possible explanation: the Monte Carlo estimate of the NLL part of the loss adds noise to the loss itself, which probably adds noise to the gradients as well. This adds up with the noise already introduced by minibatch gradient descent (are these noise source with the same size?): increasing the batch size reduces at least one source of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9ed77-fab3-4548-a39c-68ecfb4a312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_divergence_fn(q, p, _):\n",
    "    \"\"\"\n",
    "    Note: KL divergence is NOT symmetric and it is assumed\n",
    "          that the approximate posterior (variational\n",
    "          distribution) is the FIRST entry and the prior\n",
    "          is the SECOND one.\n",
    "    \"\"\"\n",
    "    return tfd.kl_divergence(q, p) / (x_train.shape[0] * 1.)\n",
    "\n",
    "\n",
    "class BayesianCNN(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Keras `Layer` object implementing a Bayesian CNN. Structure:\n",
    "      * Convolutional block (2 `Convolution2DFlipout` layers).\n",
    "      * Maxpooling.\n",
    "      * Convolutional block (2 `Convolution2DFlipout` layers).\n",
    "      * Maxpooling.\n",
    "      * Flattening.\n",
    "      * Fully connected block (2 `DenseFlipout` layers and a\n",
    "          final output one).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the internal layers.\n",
    "        self.conv_block_1 = [\n",
    "            tfp.layers.Convolution2DFlipout(\n",
    "                8,\n",
    "                kernel_size=(3, 3),\n",
    "                padding='same',\n",
    "                # activation='relu',\n",
    "                kernel_divergence_fn=kernel_divergence_fn\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        ]\n",
    "\n",
    "        self.conv_block_2 = [\n",
    "            tfp.layers.Convolution2DFlipout(\n",
    "                16,\n",
    "                kernel_size=(3, 3),\n",
    "                padding='same',\n",
    "                # activation='relu',\n",
    "                kernel_divergence_fn=kernel_divergence_fn\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        ]\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        self.dense_block = [\n",
    "            tfp.layers.DenseFlipout(\n",
    "                units=100,\n",
    "                # activation='relu',\n",
    "                kernel_divergence_fn=kernel_divergence_fn\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tfp.layers.DenseFlipout(\n",
    "                units=100,\n",
    "                # activation='relu',\n",
    "                kernel_divergence_fn=kernel_divergence_fn\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "        ]\n",
    "\n",
    "        self.output_layer = tfp.layers.DenseFlipout(units=9, activation='softmax', kernel_divergence_fn=kernel_divergence_fn)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        for conv_layer in self.conv_block_1:\n",
    "            x = conv_layer(x)\n",
    "\n",
    "        for conv_layer in self.conv_block_2:\n",
    "            x = conv_layer(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        for dense_layer in self.dense_block:\n",
    "            x = dense_layer(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d9e4c-0f50-48c2-ad96-44549fd1ad4f",
   "metadata": {
    "id": "e70d9e4c-0f50-48c2-ad96-44549fd1ad4f"
   },
   "outputs": [],
   "source": [
    "bayesian_cnn_layer = BayesianCNN()\n",
    "\n",
    "# Test.\n",
    "bayesian_cnn_layer(x_train[:15]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Byvr4KJTMIb",
   "metadata": {
    "id": "0Byvr4KJTMIb"
   },
   "outputs": [],
   "source": [
    "# Test: the final softmax activation should normalize all output vectors\n",
    "# to 1.\n",
    "tf.reduce_sum(bayesian_cnn_layer(x_train[:15, ...]), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3a8fa-6080-49b4-8c97-b8ca17bb0afc",
   "metadata": {
    "id": "dfd3a8fa-6080-49b4-8c97-b8ca17bb0afc"
   },
   "source": [
    "Build a Keras `Model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d53592-e8cc-4ea8-ac00-16cf803c5a38",
   "metadata": {
    "id": "b6d53592-e8cc-4ea8-ac00-16cf803c5a38"
   },
   "outputs": [],
   "source": [
    "# Input has shape (32, 32, 3) for RGB images and (32, 32, 1)\n",
    "# for grayscale ones.\n",
    "inputs = tf.keras.Input(shape=(32, 32, 1,))\n",
    "\n",
    "outputs = BayesianCNN()(inputs)\n",
    "\n",
    "bayesian_cnn_model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "bayesian_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7c8dd-5102-49e9-ac23-56b8663b279e",
   "metadata": {
    "id": "9fc7c8dd-5102-49e9-ac23-56b8663b279e"
   },
   "source": [
    "Multiple predictions return different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77906af1-3878-4085-9c7c-c4696ba28148",
   "metadata": {
    "id": "77906af1-3878-4085-9c7c-c4696ba28148"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(bayesian_cnn_model(x_test[:1, ...]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec00a6c-a2e9-415c-9059-bb6045bac3eb",
   "metadata": {
    "id": "9ec00a6c-a2e9-415c-9059-bb6045bac3eb"
   },
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845B3J5D8tRU",
   "metadata": {
    "id": "845B3J5D8tRU"
   },
   "outputs": [],
   "source": [
    "# Define callback to save/reload the model automatically every time\n",
    "# training ends/starts.\n",
    "backup_callback = tf.keras.callbacks.BackupAndRestore(\n",
    "    backup_dir='./models/'\n",
    ")\n",
    "\n",
    "# Recreate the model (for retraining purposes).\n",
    "inputs = tf.keras.Input(shape=x_train.shape[1:])\n",
    "\n",
    "outputs = BayesianCNN()(inputs)\n",
    "\n",
    "bayesian_cnn_model = tf.keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "bayesian_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc6d13-2e1a-4afc-97da-37a3d5ccb370",
   "metadata": {
    "id": "22dc6d13-2e1a-4afc-97da-37a3d5ccb370"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Previously tried SGD.\n",
    "\n",
    "bayesian_cnn_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k4DXFUO1k3Ig",
   "metadata": {
    "id": "k4DXFUO1k3Ig"
   },
   "outputs": [],
   "source": [
    "K.set_value(bayesian_cnn_model.optimizer.lr, 1e-5)\n",
    "\n",
    "print('New learning rate:', K.get_value(bayesian_cnn_model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51538da5-79f2-46c6-9b5a-4b1693044797",
   "metadata": {
    "id": "51538da5-79f2-46c6-9b5a-4b1693044797"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "history = bayesian_cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=x_train.shape[0],\n",
    "    callbacks=[backup_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CTevOz7QkU3B",
   "metadata": {
    "id": "CTevOz7QkU3B"
   },
   "outputs": [],
   "source": [
    "# Loss history\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(history.history['loss'])),\n",
    "    y=history.history['loss'],\n",
    "    label='Total',\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
