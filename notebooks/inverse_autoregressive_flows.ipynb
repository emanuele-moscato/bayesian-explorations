{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579a4334-5c14-4dba-810f-285eb516f39e",
   "metadata": {},
   "source": [
    "# Inverse autoregressive flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9647d5-86eb-4998-b99f-e45d6a1e4f8b",
   "metadata": {},
   "source": [
    "__Objective:__ implement an inverse autoregressive normalizing flow (NF) for 2-dimensional data, then see how moving along a trajectory in \"simple space\" reflects onto the \"complicated space\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db0f17b-7672-43f6-8475-f2213acd1664",
   "metadata": {},
   "source": [
    "Source: [here](https://github.com/tensorchiefs/dl_book/blob/master/chapter_06/nb_ch06_04.ipynb) (section \"Understanding the mixture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39d314-386d-4ab1-9d29-6501763f3ebf",
   "metadata": {},
   "source": [
    "**Idea:** define a NF parametrized by neural networks. For this to work, a number of requirements must be satisfied, in particular the flow (transformation) must be **invertible** and the Jacobian must be **lower triangular** and with diagonal components that can be computed quickly (so that the computation of the determinant is fast).\n",
    "\n",
    "As with usual NF, we start from a probability density function $p_x$ in $d$ dimensions generating points $\\mathbf{x}\\in \\mathbb{R}^d$ and we want to find a transformation $\\mathbf{G}$ whose inverse maps it to another distribution $p_z$ that has a simple form, i.e. (in the \"direct\" direction) $\\mathbf{x} = \\mathbf{G}(\\mathbf{z})$. The transformation law between the two distributions is\n",
    "$$\n",
    "p_x(x) = p_z(z)\\, \\left| \\det\\left( \\left[ \\frac{\\partial G_i}{\\partial z_j} \\right] \\right) \\right|^{-1},\n",
    "$$\n",
    "the matrix $J\\equiv \\left[ \\frac{\\partial G_i}{\\partial z_j} \\right]$ being the Jacobian of the transformation.\n",
    "\n",
    "Inverse autoregressive flows correspond to the followinf particular choice for the map $\\mathbf{G}$:\n",
    "\n",
    "$$\n",
    "G_i = G_i(z_1, \\ldots, z_i) = \\exp\\left( \\alpha_i(z_1, \\ldots, z_{i-1}) \\right)\\, z_i + b_i(z_1, \\ldots, z_{i-1}),\\quad i = 1, \\ldots, d\\,,\n",
    "$$\n",
    "\n",
    "with the \"affine parameters\" $\\alpha_i$ and $b_i$ being given by neural networks and thus containing nonlinearities.\n",
    "\n",
    "Observations:\n",
    "- $G_i$ doesn't depend on $z_j$ if $j>i$: this creates a lower triangular Jacobian.\n",
    "- $G_i$ depends on $z_i$ only in an affine way: this gives diagonal entries of $J$ that are easy to compute.\n",
    "- $G_i$ depends on the $z_i$ for $i<j$ through $\\alpha_i$ and $b_i$.\n",
    "- The exponential is used so that the diagonal entries of $J$ are all positive, which gives an invertible transformation by construction.\n",
    "\n",
    "The above form for $\\mathbf{G}$ is achieved by using \"autoregressive networks\", which are implemented in Tensorflow Probability.\n",
    "\n",
    "Tensorflow Probability provides the following implementations:\n",
    "- `tfp.bijectors.Bijector`: the general class implementing an invertible transformation (a normalizing flow).\n",
    "- `tfp.bijectors.MaskedAutoregressiveFlow`: the bijector implementing the full inverse autoregressive flow (\"masked\" refers to the fact that the NN used to get the parameters achieve the autoregressive properties via masking).\n",
    "- `tfp.bijectors.AutoregressiveNetwork`: the bijector implementing the masked NN used to parametrize the inverse autoregressive flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6180303-0585-46d0-836a-0cc6c6a09622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77a125-b948-4d19-8b86-dd459b3cb979",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c010d5-ee94-4a63-a023-f03681b6087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2500\n",
    "\n",
    "x2_samples = tfd.Normal(loc=0., scale=4.).sample(n_samples)\n",
    "\n",
    "x1_samples = tfd.Normal(loc=.25 * tf.square(x2_samples), scale=tf.ones(n_samples, dtype=tf.float32)).sample()\n",
    "\n",
    "samples = tf.stack(\n",
    "    [x1_samples, x2_samples],\n",
    "    axis=1\n",
    ") / 40.\n",
    "\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83828f4-ebfe-44d8-9b1d-3ce1c9d0e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0],\n",
    "    y=samples[:, 1],\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29b46d-63fa-446f-8020-b9a588568411",
   "metadata": {},
   "source": [
    "## Build an inverse autoregressive flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f4a79f-7627-4a9e-bffa-22d5bddc486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedARFlow(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Subclass of Keras `Model` implementing an inverse autoregressive\n",
    "    flow (using masked autoregressive networks).\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # List of the NNs giving the affine parameters.\n",
    "        self.nets = []\n",
    "        \n",
    "        # List of all bijectors that will compose the\n",
    "        # full one.\n",
    "        bijectors = []\n",
    "        \n",
    "        # Number layers, each one composed of a\n",
    "        # `MaskedAutoregressiveFLow` and a `Permute`\n",
    "        # bijector.\n",
    "        n_layers = 5\n",
    "        h = 32\n",
    "        \n",
    "        # Loop over the number of layers. Each layer is a block of bijecctors\n",
    "        # that gets added to the list.\n",
    "        for i in range(n_layers):\n",
    "            # Creates the function building the NN parametrizing the affine\n",
    "            # parameters (it's constructed so it can be passed to the\n",
    "            # `shift_and_log_scale_fn` argument of the `MaskedAutoregressiveFlow`\n",
    "            # bijector object.\n",
    "            net = tfb.AutoregressiveNetwork(\n",
    "                # This SHOULD be the number of parameters we want to generate (which\n",
    "                # in our case should be the 2 affine parameters alpha and b).\n",
    "                params=2,\n",
    "                hidden_units=[4, 4]\n",
    "            )\n",
    "            # Original code in source: doesn't work as the trainable variables are not\n",
    "            # tracked!\n",
    "            # net = tfb.masked_autoregressive_default_template(hidden_layers=[h, h])\n",
    "            \n",
    "            # Create a `MaskedAutoregressiveFlow` bijector and append it to the\n",
    "            # list of bijectors.\n",
    "            bijectors.append(\n",
    "                tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn=net)\n",
    "            )\n",
    "            \n",
    "            # Create a `Permute` bijector and append it to the list of bijectors.\n",
    "            # The permutation passed to its constructor inverts the order of the\n",
    "            # two dimensions so that no dimension is singled out in the procedure.\n",
    "            # Note: this of course works only for 2 dimensions.\n",
    "            bijectors.append(tfb.Permute([1, 0]))\n",
    "            \n",
    "            # Append the neural network to the list of the networks to keep track\n",
    "            # of it.\n",
    "            self.nets.append(net)\n",
    "            \n",
    "        # Create the full bijector defining the NF from the list of built bijectors\n",
    "        # taken in reversed order. This way, in each block the permutation operation\n",
    "        # is applied before the masked autoregressive flow. The last entry of the list\n",
    "        # is dropped so that, in the reversed list, we don't start with a useless\n",
    "        # permutation.\n",
    "        bijector = tfb.Chain(list(reversed(bijectors[:-1])))\n",
    "        \n",
    "        # Define the flow, i.e. the distribution object implementing the transformation\n",
    "        # from a source simple distribution via the full bijector.\n",
    "        self.flow = tfd.TransformedDistribution(\n",
    "            distribution=tfd.MultivariateNormalDiag(loc=[0., 0.]),\n",
    "            bijector=bijector\n",
    "        )\n",
    "        \n",
    "    def call(self, *inputs):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \"\"\"\n",
    "        return self.flow.bijector.forward(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d839ce2-4995-42c6-87e0-d0d864e58789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the source distribution and transform the samples via the\n",
    "# untrained flow. This should give nothing sensible!\n",
    "# Note: every time the model is instantiated, the NN weights\n",
    "#       in it are re-initialized and a different transformation\n",
    "#       is obtained.\n",
    "transformed_samples_untrained = MaskedARFlow()(MaskedARFlow().flow.distribution.sample(2500))\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0].numpy(),\n",
    "    y=samples[:, 1].numpy(),\n",
    "    label='Original samples'\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples_untrained[:, 0].numpy(),\n",
    "    y=transformed_samples_untrained[:, 1].numpy(),\n",
    "    label='Transformed samples (untrained flow)'\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81e43b-ac57-4259-88c4-8dc76de97360",
   "metadata": {},
   "source": [
    "Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b23831-56a7-404d-8e1f-bf293ec17ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(samples, distr):\n",
    "    \"\"\"\n",
    "    Negative log likelihood of `samples` according to the\n",
    "    distribution `distr`.\n",
    "    \"\"\"\n",
    "    return - tf.reduce_mean(distr.log_prob(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ecb36-a924-4821-8ddd-45066e5de60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedARFlow()\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c2708-14fa-4d2a-ae02-d940c9b73b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll(x, model.flow)\n",
    "        \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c7f6b-9989-48cd-ace3-cdf125e0f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50000\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n",
    "\n",
    "for i in range(epochs):\n",
    "    # with tf.GradientTape() as tape:\n",
    "    #     loss = nll(samples, model.flow)\n",
    "    loss = training_step(samples)\n",
    "        \n",
    "    loss_history.append(loss.numpy())\n",
    "\n",
    "    # grad = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    \n",
    "    if (i < 10) or (i % 100 == 0):\n",
    "        print(f'Epoch: {i} | Loss: {loss_history[-1]}')\n",
    "    \n",
    "loss_history.append(nll(samples, model.flow).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4f419-f7d4-4401-8b00-c04a791f7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history)),\n",
    "    y=loss_history\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Sample the source distribution and transform the samples via the\n",
    "# trained flow. This should give something that looks much more\n",
    "# like the original samples, as the flow should have understood\n",
    "# how to map the simple space into the complicated one.\n",
    "transformed_samples = model(model.flow.distribution.sample(2500))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=samples[:, 0].numpy(),\n",
    "    y=samples[:, 1].numpy(),\n",
    "    label='Original samples'\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples[:, 0].numpy(),\n",
    "    y=transformed_samples[:, 1].numpy(),\n",
    "    label='Transformed samples'\n",
    ")\n",
    "\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.title('Samples', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
