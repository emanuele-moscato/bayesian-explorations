{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fe0802-9ef0-4539-9ad2-222c3d853bc1",
   "metadata": {},
   "source": [
    "# RealNVP flows from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecfe3ef-2d07-4dc3-9a0a-8e930b8894d5",
   "metadata": {},
   "source": [
    "__Objective:__ build and train a simple RealNVP flow model from scratch.\n",
    "\n",
    "__Source:__ D. Foster, [_Generative deep learning_](https://www.oreilly.com/library/view/generative-deep-learning/9781492041931/) (2nd ed.) (with notebooks [here](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a88be-40d3-47bd-a7c7-ab3c43b27e9e",
   "metadata": {},
   "source": [
    "**Setup:**\n",
    "- We start from a vetor $z \\in \\mathbb{R}^D$ in latent space, which we sample from a multivariate standard normal distribution, so $p_Z \\sim \\mathcal{N}(0, I)$.\n",
    "- We transform $z$ to the \"real\" data space $x \\in \\mathbb{R}^D$ via the RealNVP transformation so that $z \\to x = x(z)$ is the **forward** transformation (this is opposite to what's done in the source, in which this is taken to be the inverse transformation, but for RealNVP's it doesn't really matter the forward and the inverse transformation are computationally equivalent).\n",
    "- The RealNVP transformation is implemented by a stack of **coupling layers** with feature permutation operations (bijectors) in between.\n",
    "- Following the RealNVP recipe, in each coupling layer the first $d$ dimensions (features) of $x$ are singled out and used to generate the corresponding dimensions of $z$ (an identity transformation) and to parametrize (via a neural network) an affine transformation for the last $(D - d)$ dimensions of $z$.\n",
    "- Full transformation for a single coupling layer:\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "z_i &=& x_i\\quad \\forall x=i, \\ldots, d\\\\\n",
    "z_j &=& x_j\\,\\exp\\left( s_j(x_1, \\ldots, x_d) \\right) + t_j(x_1, \\ldots, x_d)\\quad \\forall j = d+1, \\ldots, D\n",
    "\\end{array}\n",
    "$$\n",
    "where the vectors $s, t \\in \\mathbb{R}^{D-d}$ are the tensors outputted by the coupling layer and are functions of $x_1, \\ldots, x_d$ given by a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d342c-ee10-4560-bd61-6679531ebd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../modules/')\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ba07d-6412-4bbb-ac62-3ae80465df1b",
   "metadata": {},
   "source": [
    "## Coupling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1200c64c-1a73-42e3-be97-f7e2143f935a",
   "metadata": {},
   "source": [
    "The coupling layer is responsible for taking the first $d$ dimensions (features) of the input and outputting a scale and a translation tensor (so two outputs) to be used to parametrize an affine transformation for the remaining $(D - d)$ dimensions of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59e72e-d459-4bcf-971a-108b3b63a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_nvp import CouplingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd702ed2-326a-4c3c-a9de-0dc9e6c16649",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_masked_dims = 2\n",
    "n_affine_dims = 3\n",
    "\n",
    "test_cl = CouplingLayer(\n",
    "    n_masked_dims=n_masked_dims,\n",
    "    n_affine_dims=n_affine_dims,\n",
    "    hidden_layers_dims=[32, 32]\n",
    ")\n",
    "\n",
    "s, t = test_cl(tf.random.normal(shape=(14, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83739fea-b1aa-477f-8100-67b9eac47121",
   "metadata": {},
   "source": [
    "## RealNVP bijector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43425c-b818-4844-82e9-9df78a5b2b15",
   "metadata": {},
   "source": [
    "Parametrize an affine (scale and then shift) tranformation with the output from the `CouplingLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ca7bd2-2cbe-4c66-b012-34af3d9ab201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_nvp import AffineBijector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8174f55-b6bf-48ea-847e-201388fb09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple test data.\n",
    "test_data = tf.ones(shape=(4, 5)) * 2.4\n",
    "\n",
    "# Instantiate an affine bijector parametrized by a\n",
    "# coupling layer so that it works as a RealNVP bijector.\n",
    "test_real_nvp_bij = AffineBijector(test_cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d79bcc-4fa9-46c4-8601-252d5b631d4b",
   "metadata": {},
   "source": [
    "Test the forward and inverse transformations. In both cases the first `n_masked_dims` of the datapoints should be left unaltered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbcdc92-3c9b-4341-bea0-75da142ab941",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_nvp_bij.forward(test_data), test_real_nvp_bij.inverse(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd284e-775c-42e0-aa38-796be8c4160c",
   "metadata": {},
   "source": [
    "Check a \"cycle condition\": applying the forward and then the inverse transformation on some data (and vice versa) we should reobtain the starting tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f3726-9f9b-4eac-8c03-0f886320e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(test_real_nvp_bij.inverse(test_real_nvp_bij.forward(test_data)) - test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e135fa-3fb5-4f50-bfcc-ca4e27b108d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.norm(test_real_nvp_bij.forward(test_real_nvp_bij.inverse(test_data)) - test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c11627-be34-4397-b83b-b6638d5ead40",
   "metadata": {},
   "source": [
    "## RealNVP layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6c471f-3972-4a54-92d1-3dafa5de85a6",
   "metadata": {},
   "source": [
    "The `RealNVP` layer object represents one RealNVP block inside a larger model. It's composed of 2 operations: a feature permutation followed by an affine transformation parametrized by a `CouplingLayer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038041da-b253-4f0c-b62e-d69cabe6a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_nvp import RealNVPLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad554e-79e5-4f3d-bf2a-8f4898212edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_layer = RealNVPLayer(\n",
    "    n_masked_dims=2,\n",
    "    n_affine_dims=3,\n",
    "    hidden_layers_dims=[32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5805e-49b1-4ddf-bf22-6178c58c83e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_layer(\n",
    "    tf.constant([range(test_rnvp_layer.n_masked_dims + test_rnvp_layer.n_affine_dims)] * 4, dtype=tf.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60e10c-a4d4-42bc-8a81-7f4acfe44b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_layer.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d0504-0d08-43a5-b7df-ff4435f0e14c",
   "metadata": {},
   "source": [
    "## RealNVP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebcb72-02cc-4679-b3cf-640d903458a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from real_nvp import RealNVPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cd8c3-a9be-4035-8f0f-29e2254dcb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_model = RealNVPModel(\n",
    "    n_masked_dims=2,\n",
    "    n_affine_dims=3,\n",
    "    n_real_nvp_blocks=3,\n",
    "    hidden_layers_dims=[32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90ffeb-e601-4113-9a42-031b473c96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_model(test_data)\n",
    "\n",
    "test_rnvp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50a13a-5f1b-4ce2-b33d-74dbcf234d92",
   "metadata": {},
   "source": [
    "Compute log probabilities with the base and the transformed distributions.\n",
    "\n",
    "**Note:** the event shape of the distribution should be equal to the shape of one sample so that one sample corresponds to one value of log prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46354be9-31d1-4604-b3f4-add92cb9427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_model.base_distr, test_rnvp_model.transformed_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf61187-408a-408d-85b6-6c8bd2d2470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rnvp_model.base_distr.log_prob(test_data), test_rnvp_model.transformed_distr.log_prob(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aaf158-1921-4526-a819-fd3a14068e50",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0f4f6-69ef-459f-8608-c55734124b4f",
   "metadata": {},
   "source": [
    "Generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe226d-49b7-4d9a-bf15-dbc7f74c4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from tensorflow.keras import backend as K\n",
    "from keras_utilities import plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3ef95-52e7-4304-ac3c-074cb13e6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels_data = make_moons(n_samples=2500, noise=.06)\n",
    "\n",
    "data = tf.constant(data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578476a-a954-4159-9a90-7df97e675e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnvp_model = RealNVPModel(\n",
    "    n_masked_dims=1,\n",
    "    n_affine_dims=1,\n",
    "    n_real_nvp_blocks=3,\n",
    "    hidden_layers_dims=[32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c7091-216a-4f8d-b8cf-7f15124804eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(14, 6), nrows=1, ncols=2)\n",
    "\n",
    "base_samples = rnvp_model.base_distr.sample(2500)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=base_samples[:, 0],\n",
    "    y=base_samples[:, 1],\n",
    "    ax=axs[0]\n",
    ")\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Base distribution', fontsize=14)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=data[:, 0],\n",
    "    y=data[:, 1],\n",
    "    ax=axs[1],\n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "transformed_samples = rnvp_model.transformed_distr.bijector.forward(base_samples)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples[:, 0],\n",
    "    y=transformed_samples[:, 1],\n",
    "    ax=axs[1],\n",
    "    label='Transformed samples (before training)',\n",
    "    alpha=.2\n",
    ")\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.xlabel('x')\n",
    "plt.title('Data', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6e793-b230-4e7a-8204-55b40a0fce2e",
   "metadata": {},
   "source": [
    "Fit model to the data using a custom training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f272b40-de1d-495f-9e03-d4f993e467f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(data, distr):\n",
    "    \"\"\"\n",
    "    Negative log likelihood of data `data` w.r.t. the distribution\n",
    "    `distr`, used as an objective function (loss) to minimize\n",
    "    during training.\n",
    "    \"\"\"\n",
    "    return - tf.reduce_mean(distr.log_prob(data))\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(data, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    A single training step.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll(data, model.transformed_distr)\n",
    "    \n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f39838-2906-46bd-b2f9-2f512b2c2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = {\n",
    "    'loss': [],\n",
    "    'learning_rate': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13d65d-e607-4950-b255-e6a94a904e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[2500],\n",
    "    values=[1e-2, 1e-3]\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a875799-b9bb-4751-b8c8-3ff70d3b4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.set_value(optimizer.learning_rate, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd5779-506a-464b-afb6-2709a6cba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    training_history['loss'].append(training_step(data, rnvp_model, nll, optimizer).numpy())\n",
    "\n",
    "    training_history['learning_rate'].append(optimizer.learning_rate.numpy())\n",
    "\n",
    "    if (epoch < 5) or (epoch % 200 == 0):\n",
    "        print(f'Epoch: {epoch} | Loss: {training_history[\"loss\"][-1]} | Learning rate: {training_history[\"learning_rate\"][-1]}')\n",
    "\n",
    "plot_history(training_history)\n",
    "\n",
    "\n",
    "# Plot data and samples from the transformed distribution.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "transformed_samples = rnvp_model.transformed_distr.sample(2500)\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=data[:, 0].numpy(),\n",
    "    y=data[:, 1].numpy(),\n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=transformed_samples[:, 0],\n",
    "    y=transformed_samples[:, 1],\n",
    "    label='Transformed samples (after training)',\n",
    "    alpha=.2\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Data', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
