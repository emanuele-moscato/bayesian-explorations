{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2319525e-4e8f-4314-9d01-496fb50379d9",
   "metadata": {},
   "source": [
    "# Bayesian linear regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c958c3-f9e5-4614-99ed-3cb935380f0e",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- Use MCMC sampling to perform a Bayesian version of standard linear regression.\n",
    "- Repeat the same trying to fit a piecewise linear function to some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de49b2c-85f2-4d7c-8c3a-4ab92ee8bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import arviz as az\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0e193-2167-438b-97ae-f250bed5856b",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00becc83-5c04-4b59-ad17-621610f61c93",
   "metadata": {},
   "source": [
    "We'll generate synthetic data for the fit. The data is normally distributed around a straight line. To make things more interesting we'll write a single joint distribution for everything: the parameters of the distribution of the data (slope, intercept and variance of the Normal distribution) and the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3957c-5e0a-4e4c-bba0-e3828fec0f68",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d7157-4be1-4f99-91b6-54941bd0edff",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100\n",
    "\n",
    "joint_distr_synthetic = tfd.JointDistributionSequential([\n",
    "    tfd.Uniform(\n",
    "        low=-10.5 * tf.ones(shape=n_points), high=23. * tf.ones(shape=n_points)\n",
    "    ),  # x coordinates of the datapoints.\n",
    "    tfd.Normal(loc=2.5, scale=3.),  # m\n",
    "    tfd.Uniform(low=-5., high=12.),  # q\n",
    "    tfd.TransformedDistribution(\n",
    "        tfd.HalfNormal(scale=.5),\n",
    "        tfp.bijectors.Shift(shift=10.)),  # sigma\n",
    "    lambda sigma, q, m, x_data: tfd.Independent(\n",
    "        tfd.Normal(loc=x_data * m + q, scale=sigma),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "])\n",
    "\n",
    "# Sample the joint distribution.\n",
    "distr, samples = joint_distr_synthetic.sample_distributions()\n",
    "\n",
    "x_data, m_sampled, q_sampled, sigma_sampled, y_data = samples\n",
    "\n",
    "# Plot the samples.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710238f-ff9c-4932-8088-90cb90c57bff",
   "metadata": {},
   "source": [
    "### Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bc653-b7b4-4521-a90f-4d3b5b7f2bc5",
   "metadata": {},
   "source": [
    "We start by writing our Bayesian model, i.e. a distribution that descirbes how we think the data was generated, including the priors from the parametres (and pretending we never saw the distribution that generated the data to begin with!). This is our modelling hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc4ea0-ba99-4f29-adc4-41bb3a2539a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_stuff(states, previous_kernel_results):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # I couldn't find a way not to make the counter global.\n",
    "    step = next(counter)\n",
    "    \n",
    "    if (step % 100) == 0:\n",
    "        print(f\"Step {step}, state: {states}\")\n",
    "    \n",
    "    return previous_kernel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c0c0f-61a7-49fe-86a3-c18965075bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_prob = tfd.JointDistributionSequential([\n",
    "    tfd.Normal(loc=0., scale=20.),  # Prior for m.\n",
    "    tfd.Normal(loc=0, scale=30.),  # Prior for q.\n",
    "    tfd.Uniform(low=0., high=50.),  # Prior for sigma.\n",
    "    # Note: the Independent distribution here is needed so a single sample\n",
    "    #       of this distribution corresponds to the whole dataset, which means\n",
    "    #       that given values for m, q, sigma and x and y coordinated of the\n",
    "    #       datapoints, a call to the log_prob method returns a scalar.\n",
    "    # Note: the batch size is kept nontrivial, which is the way we deal with\n",
    "    #       sampling from multiple chains in parallel.\n",
    "    lambda sigma, q, m: tfd.Independent(\n",
    "        tfd.Normal(\n",
    "            loc=tf.transpose(tf.expand_dims(x_data, 1)) * tf.expand_dims(m, -1) + tf.expand_dims(q, -1),\n",
    "            scale=tf.expand_dims(sigma, -1)\n",
    "        ),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "])\n",
    "\n",
    "joint_log_prob_closure = (\n",
    "    lambda m, q, sigma: joint_prob.log_prob(m, q, sigma, y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c3da9-b28d-4e3a-a26b-64677447a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the closure of the joint log prob correclty evaluates potential\n",
    "# values for m, q and sigma.\n",
    "n_chains = 4\n",
    "\n",
    "test_state = [\n",
    "    1. * tf.ones(shape=n_chains),\n",
    "    1. * tf.ones(shape=n_chains),\n",
    "    1. * tf.ones(shape=n_chains),\n",
    "]\n",
    "\n",
    "joint_log_prob_closure(*test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9239f-cb78-4ab1-82c6-ed49136adb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the chain's start state using a the frequentist statistics estimators.\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(x_data.numpy().reshape(-1, 1), y_data.numpy())\n",
    "\n",
    "max_lik_est_m = tf.constant(lr.coef_[0])\n",
    "max_lik_est_q = tf.constant(lr.intercept_)\n",
    "\n",
    "residuals = (\n",
    "    y_data\n",
    "    - (x_data * max_lik_est_m + max_lik_est_q)\n",
    ")\n",
    "\n",
    "max_likest_sigma = tf.sqrt(tf.reduce_sum(residuals * residuals) / (n_points - 2))\n",
    "\n",
    "\n",
    "initial_chain_state = [    \n",
    "    max_lik_est_m * tf.ones(shape=n_chains),\n",
    "    max_lik_est_q * tf.ones(shape=n_chains),\n",
    "    max_likest_sigma * tf.ones(shape=n_chains)\n",
    "]\n",
    "\n",
    "initial_chain_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05646af7-b144-41bb-ac65-15e0bdefed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frequentist linear regression.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "data_max_lik_fit = tf.sort(tf.transpose(tf.stack([\n",
    "    x_data, x_data * max_lik_est_m + max_lik_est_q])), axis=0)\n",
    "\n",
    "data_max_lik_fit\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    color='b',\n",
    "    label='Data')\n",
    "\n",
    "plt.fill_between(\n",
    "    x=data_max_lik_fit[:, 0].numpy(),\n",
    "    y2=(data_max_lik_fit[:, 1] - 2. * max_likest_sigma).numpy(),\n",
    "    y1=(data_max_lik_fit[:, 1] + 2. * max_likest_sigma).numpy(),\n",
    "    alpha=0.3,\n",
    "    color='g',\n",
    "    label='2-$\\sigma$ band'\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    data_max_lik_fit[:, 0].numpy(),\n",
    "    data_max_lik_fit[:, 1].numpy(),\n",
    "    color='r',\n",
    "    label='Maximum likelihood fit')\n",
    "\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e1ff9-9530-45a8-992d-f117da8be3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 5000\n",
    "burnin = 1000\n",
    "leapfrog_steps=3\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Identity(),  # Maps R to R (m).\n",
    "    tfp.bijectors.Identity(),  # Maps R to R (q).\n",
    "    tfp.bijectors.Exp(),  # Maps R to (0, +oo) (sigma).\n",
    "]\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "step_size = tf.Variable(0.5, dtype=tf.float32)\n",
    "\n",
    "# Defining the HMC\n",
    "hmc=tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=joint_log_prob_closure,\n",
    "        num_leapfrog_steps=3,\n",
    "        step_size=step_size,\n",
    "        step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "        state_gradients_are_stopped=True),\n",
    "    bijector=unconstraining_bijectors)\n",
    "\n",
    "# Sample from the chain.\n",
    "print('Sampling started')\n",
    "\n",
    "counter = itertools.count(1)\n",
    "\n",
    "[\n",
    "    trace_m,\n",
    "    trace_q,\n",
    "    trace_sigma\n",
    "], kernel_results = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps + burnin,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=trace_stuff\n",
    ")\n",
    "\n",
    "print('Sampling finished')\n",
    "\n",
    "trace_m_burned = trace_m[burnin:, :]\n",
    "trace_q_burned = trace_q[burnin:, :]\n",
    "trace_sigma_burned = trace_sigma[burnin:, :]\n",
    "\n",
    "posterior_means = {\n",
    "    'm': tf.reduce_mean(trace_m_burned, axis=0),\n",
    "    'q': tf.reduce_mean(trace_q_burned, axis=0),\n",
    "    'sigma': tf.reduce_mean(trace_sigma_burned, axis=0)}\n",
    "\n",
    "inference_data = az.convert_to_inference_data({\n",
    "    'm': tf.transpose(trace_m_burned),\n",
    "    'q': tf.transpose(trace_q_burned),\n",
    "    'sigma': tf.transpose(trace_sigma_burned)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528564f-16cf-4092-8197-6d456545439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_results.inner_results.is_accepted.numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8e21a-cac6-4247-8122-73bc2a4ee40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784a6c7-cf4b-4442-af66-4525b4582148",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sampled, q_sampled, sigma_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d9218-8d45-407b-ae1a-58ac865ddfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2258f1-ee51-4c29-ac5b-d2ddbc53131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd90b2-f254-40ed-85ac-7c50e9801130",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(inference_data)\n",
    "\n",
    "az.plot_autocorr(inference_data)\n",
    "\n",
    "az.plot_posterior(inference_data)\n",
    "\n",
    "az.plot_forest(inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71fc11e-725a-4d45-a6fe-118c98de473b",
   "metadata": {},
   "source": [
    "An example of autocorrelation with thinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3c7c1-cc3a-4bb1-938d-be6bf37b42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(trace_m_burned[:, 0].numpy().T)\n",
    "\n",
    "plt.title('Autocorrelation without thinning')\n",
    "\n",
    "az.plot_autocorr(trace_m_burned[::3, 0].numpy().T)\n",
    "\n",
    "plt.title('Autocorrelation with thinning (keeping 1 sample every 3)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e69c8-7ba4-4e9c-83ce-5068a698b8a1",
   "metadata": {},
   "source": [
    "Plot the line corresponding to the mean of the posterior samples for each parameter and another line corresponding to other porterior samples drawn randomly from the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55d767-8776-4163-af41-2fdd111f9057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred(x, m, q):\n",
    "    return x * m + q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a4c5b-f6d1-4d0c-b8e4-984f0eb087bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_m_burned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80491b-cf8f-408c-80d3-c7b8b17b6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(x_data.numpy().min(), x_data.numpy().max(), 100)\n",
    "y_plot = compute_pred(\n",
    "    x_plot,\n",
    "    tf.reduce_mean(tf.concat([\n",
    "        trace_m_burned[:, 0],\n",
    "        trace_m_burned[:, 1],\n",
    "        trace_m_burned[:, 2],\n",
    "        trace_m_burned[:, 3]\n",
    "    ], axis=-1)),\n",
    "    tf.reduce_mean(tf.concat([\n",
    "        trace_q_burned[:, 0],\n",
    "        trace_q_burned[:, 1],\n",
    "        trace_q_burned[:, 2],\n",
    "        trace_q_burned[:, 3]\n",
    "    ], axis=-1)),\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    label='Data')\n",
    "\n",
    "n_param_samples = 50\n",
    "\n",
    "chain_indices = np.random.choice(trace_m_burned.shape[1], n_param_samples)\n",
    "sample_indices = np.random.choice(trace_m_burned.shape[0], n_param_samples)\n",
    "\n",
    "for si, ci in zip(sample_indices, chain_indices):\n",
    "    plt.plot(\n",
    "        x_plot,\n",
    "        compute_pred(x_plot, trace_m_burned[si, ci], trace_q_burned[si, ci]),\n",
    "        color='g',\n",
    "        alpha=.5\n",
    "    )\n",
    "    \n",
    "plt.plot(\n",
    "    x_plot,\n",
    "    y_plot,\n",
    "    color='r',\n",
    "    label='Fit with posterior sample means')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bee8ee-5996-4fe3-b9b7-d69026de7e51",
   "metadata": {},
   "source": [
    "Generate a synthetic dataset using the means of the posterior distributions or a random sample from the posterior distribution as values for the parameters ($m$, $q$ and $\\sigma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf805ca8-0491-4eff-ae96-dcda2915cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_dataset(x_data, m, q, sigma):\n",
    "    synth_data_distr = tfd.Independent(\n",
    "        tfd.Normal(\n",
    "            loc=x_data * m + q,\n",
    "            scale=sigma),\n",
    "        reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    return synth_data_distr.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dc6ae0-30d5-4217-bff2-a5dd8fd976d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=y_data,\n",
    "    label='Data',)\n",
    "\n",
    "m_synthetic_data = tf.reduce_mean(tf.concat([\n",
    "    trace_m_burned[:, 0],\n",
    "    trace_m_burned[:, 1],\n",
    "    trace_m_burned[:, 2],\n",
    "    trace_m_burned[:, 3]\n",
    "], axis=-1))\n",
    "q_synthetic_data = tf.reduce_mean(tf.concat([\n",
    "    trace_q_burned[:, 0],\n",
    "    trace_q_burned[:, 1],\n",
    "    trace_q_burned[:, 2],\n",
    "    trace_q_burned[:, 3]\n",
    "], axis=-1))\n",
    "sigma_synthetic_data = q_synthetic_data = tf.reduce_mean(tf.concat([\n",
    "    trace_sigma_burned[:, 0],\n",
    "    trace_sigma_burned[:, 1],\n",
    "    trace_sigma_burned[:, 2],\n",
    "    trace_sigma_burned[:, 3]\n",
    "], axis=-1))\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=generate_synthetic_dataset(x_data, m_synthetic_data, q_synthetic_data, sigma_synthetic_data),\n",
    "    label='Synthetic data (posterior means)')\n",
    "\n",
    "chain_index = np.random.choice(trace_m_burned.shape[1])\n",
    "sample_index = np.random.choice(trace_m_burned.shape[0])\n",
    "\n",
    "plt.scatter(\n",
    "    x=x_data,\n",
    "    y=generate_synthetic_dataset(\n",
    "        x_data,\n",
    "        trace_m_burned[sample_index, chain_index],\n",
    "        trace_q_burned[sample_index, chain_index],\n",
    "        trace_sigma_burned[sample_index, chain_index]),\n",
    "    label='Synthetic data (random posterior sample)')\n",
    "\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
