{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c46a9b-a5b2-482d-9fc7-3f36a3b5b162",
   "metadata": {},
   "source": [
    "# Learnable normalizing flows (NF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d2055-098d-4c78-b09e-3d9e774bdcbd",
   "metadata": {},
   "source": [
    "Normalizing flows (NF) are transformations $g$ mapping a complicated distribution $p_z(z)$ representing the data to a simple one $p_x(x)$ we can easily sample from:\n",
    "\n",
    "$$\n",
    "x = g(z)\\,.\n",
    "$$\n",
    "\n",
    "$g$ needs to be invertible ($z = g^{-1}(x)$) and is implemented via TFP's `Bijector` objects.\n",
    "\n",
    "NFs can have learnable parameters and can be fitted to the data via maximum likelihood: this way we learn the best transformation between the two distributions, within the parametric family of transformations we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6e465-afec-4fbc-93b9-62bcbcc22d9e",
   "metadata": {},
   "source": [
    "Source: [here](https://github.com/tensorchiefs/dl_book/blob/master/chapter_06/nb_ch06_03.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c715d8e-c035-427c-9d0e-aa7099e0408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbb1ca9-16ba-402f-a4ce-54f89ab6d9ec",
   "metadata": {},
   "source": [
    "## An affine mapping between Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed4a15-3c58-41b2-bffb-4cc62acb2575",
   "metadata": {},
   "source": [
    "Let's generate samples from two Gaussian distributions with different parameters and learn the NF transforming one into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac85132d-cf45-42aa-828f-ed7a8529f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "standard_gaussian_samples = tfd.Normal(loc=0., scale=1.).sample(n_samples)\n",
    "generic_gaussian_samples = tfd.Normal(loc=5., scale=0.2).sample(n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165b04a-5d5e-4d24-8817-fd5dcdefce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=standard_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Standard Gaussian samples'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=generic_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generic Gaussian samples'\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b406c-ab0e-4e47-827e-bcffad9aab18",
   "metadata": {},
   "source": [
    "Define an affine bijector (implementing a linear transformation between samples) depending on two trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14155e87-12c0-4bf3-b4b1-e293b9f1b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial values for the parameters (scale and shift)\n",
    "# of the affine transformation.\n",
    "m = tf.Variable(.5, name='m')\n",
    "q = tf.Variable(1.2, name='q')\n",
    "\n",
    "affine_bij = tfp.bijectors.Chain([\n",
    "    tfp.bijectors.Shift(shift=q),\n",
    "    tfp.bijectors.Scale(scale=m)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd07c1b-57ce-46a8-95f5-442a33009508",
   "metadata": {},
   "source": [
    "Define the transformed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611f0799-64e7-4f42-833f-86b392470449",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_distr = tfd.TransformedDistribution(\n",
    "    distribution=tfd.Normal(loc=0., scale=1.),  # Source distribution.\n",
    "    bijector=affine_bij\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f8068-6561-499c-85c4-926f970733a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=standard_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Standard Gaussian samples'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=generic_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generic Gaussian samples'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=transformed_distr.sample(10000).numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[2],\n",
    "    label='Samples from the transformed distribution',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea288afa-3b87-4fc1-af5f-664a78a56a11",
   "metadata": {},
   "source": [
    "The loss function to minimize w.r.t. the variable in the NF (affine bijector) is the negative log likelihood of the target data given by the transformed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554036d-76aa-433d-a132-987955dee045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(samples, distr):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return - tf.reduce_mean(\n",
    "        distr.log_prob(samples)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b18d59-b8d7-44fd-aca0-4a7ab2b8b203",
   "metadata": {},
   "source": [
    "Training: we apply gradient descent to minimize the loss function w.r.t. the NF variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35707034-5adc-4de4-812b-15678f636b01",
   "metadata": {},
   "source": [
    "__Note:__ by trial and error it's evident that a bigger (~0.1) learning rate is needed for the first ~800 epochs, after which if the learning rate itself is kept constant, SGD starts overshooting the minimum of the loss, in which case the final value depends on where in the \"overshooting cycle\" the training ends. Decreasing the learning rate needs after epoch 800 via a decay schedule allows the minumum to be found (first 800 epochs) and the overshooting to be kept at bay (afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e480907-3703-4531-aeea-e4645fed2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "# n_lr_values = 5\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[800],  # list(tf.cast(tf.linspace(0, epochs - 1, n_lr_values + 1), dtype=tf.int64)[1:-1].numpy()),\n",
    "    values=[0.1, 0.05]  # list(tf.linspace(0.1, 0.01, n_lr_values).numpy())\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
    "\n",
    "loss_history = []\n",
    "params_history = [[m.numpy(), q.numpy()]]\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll(generic_gaussian_samples, transformed_distr)\n",
    "        \n",
    "    loss_history.append(loss.numpy())\n",
    "    \n",
    "    grad = tape.gradient(loss, [m, q])\n",
    "    \n",
    "    optimizer.apply_gradients(zip(grad, [m, q]))\n",
    "    \n",
    "    params_history.append([m.numpy(), q.numpy()])\n",
    "\n",
    "loss_history.append(nll(generic_gaussian_samples, transformed_distr).numpy())\n",
    "\n",
    "params_history = tf.constant(params_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66586b-b399-4d18-a2fb-39249faf49ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history)),\n",
    "    y=loss_history\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408eefc-7571-4d54-912d-d9ebd809ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_history[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caec61-1914-4a87-82f7-586d103b2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=standard_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Standard Gaussian samples'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=generic_gaussian_samples.numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Generic Gaussian samples'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=transformed_distr.sample(10000).numpy(),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[2],\n",
    "    label='Samples from the transformed distribution after training',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6166ef2-2351-479c-8bdc-a1cd06a4a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=params_history[::10, 0].numpy(),\n",
    "    y=params_history[::10, 1].numpy(),\n",
    "    alpha=tf.linspace(0.1, 1.0, params_history[::10, 0].shape[0]).numpy(),\n",
    "    label=\"Parameters' values\"\n",
    ")\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=params_history[-1:, 0].numpy(),\n",
    "    y=params_history[-1:, 1].numpy(),\n",
    "    color='red',\n",
    "    label='Final values'\n",
    ")\n",
    "\n",
    "plt.title(\"Parameters' trajectory along training\", fontsize=14)\n",
    "plt.xlabel('m')\n",
    "plt.ylabel('q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea2ce4-103c-47fb-a52a-721ff5daa5cc",
   "metadata": {},
   "source": [
    "## A more complicated stack of NF, with nonlinearities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a635a-c390-4e2c-9227-85bfc5d6f5e2",
   "metadata": {},
   "source": [
    "Load the Old Faithful dataset and fit the data with a more complicated stack of NF containing nonlinearities. We'll work on the `TimeWaiting` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f3d10-85c2-42a9-b765-2e946b7b3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda42a35-2e91-4fa2-a3ff-3d557435934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_faithful_data = pd.read_csv('../data/learnable_normalizing_flows/OldFaithful.csv')\n",
    "\n",
    "old_faithful_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33d197-3539-4b07-890d-9d8f9ddde6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_data = old_faithful_data.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c41ea0-2e60-4222-a794-2068dece7a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=of_data,\n",
    "    stat='density'\n",
    ")\n",
    "\n",
    "plt.title('Old Faithful TimeWaiting data', fontsize=14)\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1c5a0f-3d40-49d5-9adb-74dd000e0c53",
   "metadata": {},
   "source": [
    "Define a stack of bijectors. Each \"layer\" in the sequence is itself composed by sub-sequence of two bijectors, in order of application:\n",
    "- A `SinhArcsinh` bijector.\n",
    "- An affine bijector.\n",
    "\n",
    "__Note:__ the order in which the bijector appear in the sequence is inverted w.r.t. to the one in which they act!\n",
    "\n",
    "The source distribution will be a standard Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b3266-0d45-4ee1-b86f-d04e3021fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 5\n",
    "\n",
    "bij_list = []\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # Add an affine bijector.\n",
    "    shift = tf.Variable(0., name=f'shift_{i}')\n",
    "    scale = tf.Variable(1., name=f'scale_{i}')\n",
    "\n",
    "    bij_list.append(tfp.bijectors.Chain([\n",
    "        tfp.bijectors.Shift(shift=shift),\n",
    "        tfp.bijectors.Scale(scale=scale)\n",
    "    ]))\n",
    "\n",
    "    # Add a `SinhArcsinh` bijector.\n",
    "    skewness = tf.Variable(0., name=f'skewness_{i}')\n",
    "    tailweight = tf.Variable(1., name=f'tailweight_{i}')\n",
    "    \n",
    "    bij_list.append(tfp.bijectors.SinhArcsinh(\n",
    "        skewness=skewness,\n",
    "        tailweight=tailweight\n",
    "    ))\n",
    "    \n",
    "bij_stack = tfp.bijectors.Chain(bij_list)\n",
    "\n",
    "trainable_distr = tfd.TransformedDistribution(\n",
    "    distribution=tfd.Normal(loc=0., scale=1.),\n",
    "    bijector=bij_stack\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75231547-e634-42c0-9af0-5f9d83bc7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=of_data,\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=trainable_distr.sample(100),\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Samples from the trainable distribution (before training)'\n",
    "\n",
    ")\n",
    "\n",
    "plt.title('Old Faithful TimeWaiting data', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a58c4-da8f-41bd-b80d-03383d080d13",
   "metadata": {},
   "source": [
    "Training.\n",
    "\n",
    "__Note:__ training is extremely sensitive to the learning rate, so we need to proceed slowly and with a lot of epochs. I still couldn't reach a really optimal value (the trained distribution still failed to model the left peak in the data) - probably something more could be done with the bijectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24563691-3dc2-42d7-bb9d-3e547f110f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history_2 = []\n",
    "params_history_2 = [[var.numpy() for var in trainable_distr.trainable_variables]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb5a64b-3101-436d-a654-50579f1a91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "\n",
    "optimizer_2 = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "\n",
    "for i in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = nll(of_data, trainable_distr)\n",
    "        \n",
    "    loss_history_2.append(loss.numpy())\n",
    "    \n",
    "    grad = tape.gradient(loss, trainable_distr.trainable_variables)\n",
    "    \n",
    "    optimizer_2.apply_gradients(zip(grad, trainable_distr.trainable_variables))\n",
    "    \n",
    "    params_history_2.append([var.numpy() for var in trainable_distr.trainable_variables])\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch: {i} | Loss: {loss}')\n",
    "\n",
    "loss_history_2.append(nll(of_data, trainable_distr).numpy())\n",
    "\n",
    "# params_history_2 = tf.constant(params_history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b483f35-543b-4056-9fac-57ae23dc8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.lineplot(\n",
    "    x=range(len(loss_history_2))[100:],\n",
    "    y=loss_history_2[100:]\n",
    ")\n",
    "\n",
    "plt.title('Training loss', fontsize=14)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e7353-8f40-409d-92ca-5e79a4517302",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_distr.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c44c2c-0f7a-44e3-b886-a44153cc5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = trainable_distr.sample(100)\n",
    "\n",
    "# test_samples = test_samples[~tf.math.is_inf(test_samples)]\n",
    "test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624bdd6-d127-417e-87d7-274ba6606de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=of_data,\n",
    "    stat='density',\n",
    "    color=sns.color_palette()[0],\n",
    "    label='Data'\n",
    ")\n",
    "\n",
    "x_plot = tf.linspace(of_data.min(), of_data.max(), 100).numpy()\n",
    "\n",
    "sns.lineplot(\n",
    "    x=x_plot,\n",
    "    y=trainable_distr.prob(x_plot).numpy(),\n",
    "    color=sns.color_palette()[1],\n",
    "    label='Trained distribution'\n",
    ")\n",
    "\n",
    "plt.title('Old Faithful TimeWaiting data', fontsize=14)\n",
    "plt.xlabel('x')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
