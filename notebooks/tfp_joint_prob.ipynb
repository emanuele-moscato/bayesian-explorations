{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfe7388-7dce-492b-ae07-9db330684f32",
   "metadata": {},
   "source": [
    "# Representing joint probabilities in TensorFlow Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd9155-bd51-4771-a2fb-5f15e876f6d2",
   "metadata": {},
   "source": [
    "**Objective:** in this notebook we explore various methods to write a joint probability with TensorFlow Probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1583d-4222-45e0-b84a-f655a7810a47",
   "metadata": {},
   "source": [
    "Two of the main ingredients of Bayesian methods are the likelihood function and the prior distribution. In particular:\n",
    "- Likelihood: expresses the probability of having observed the data given certain values of the parameters of the corresponding probability density function.\n",
    "- Prior: expresses the probability density for the values of the parameters of the probability density function (and is thus defined over the space of the parameters).\n",
    "\n",
    "Given some data $\\mathcal{D}$, the likelihood is a function $p(\\mathcal{D} | \\alpha)$, where $\\alpha$ is the set of parameters, while the prior is $p(\\alpha)$. The fundamental equation of Bayesian modeling is Bayes' theorem,\n",
    "\n",
    "$$\n",
    "p(\\alpha | \\mathcal{D}) = \\frac{p(\\mathcal{D} | \\alpha)\\, p(\\alpha)}{p(\\mathcal{D})}\\,.\n",
    "$$\n",
    "\n",
    "The numerator on the RHS is the product of the likelihood and the prior, which, by the chain rule of probabilities, also expresses the joint probability for the data and the parameters, $p(\\mathcal{D} | \\alpha)\\, p(\\alpha) = p(\\mathcal{D}, \\alpha)$.\n",
    "\n",
    "The denominator is a normalization constant and in fact numerical methods like MCMC do not require its explicit computation (which can usually be done only with numerical approximations anyway). We can ignore it.\n",
    "\n",
    "Numerical methods do require the evaluation of the numerator though, so it's important to be able to write and evaluate likelihoods when using a probabilistic programming language like TensorFlow Probability (TFP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5d981d-ec81-4527-ac4b-2fce3117f54b",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51d3cd-a7e9-4e52-8e43-0a27d9721812",
   "metadata": {},
   "source": [
    "Specifying a model equates to specifying its joint probability distribution. As an example to work with, let's consider `N=100` random values $\\lbrace x_1, \\ldots, x_N\\rbrace$ sampled from a Gaussian distribution with parameters $(\\mu, \\sigma)$ with a Gaussian prior on $\\mu$ and a Half-normal prior on $\\sigma$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mu &\\sim& \\mathcal{N}(\\mu_\\mu, \\sigma_\\mu), \\\\\n",
    "\\sigma &\\sim& \\mathcal{HN}(0, \\sigma_\\sigma), \\\\\n",
    "x_i &\\sim& \\mathcal{N}(\\mu, \\sigma)\\quad \\forall i = 1, \\ldots, N\\,.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The prior themselves depend on the choice on some parameters that we need to specify (we could put a prior on those as well, but at some point some parameter will need to be specified). In particular we can set\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "(\\mu_\\mu, \\sigma_\\mu) &=& (15.0, 2.0), \\\\\n",
    "\\sigma_\\sigma &=& 1.0\\,.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We thus have\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(\\mathcal{D} | \\alpha) &=&  \\prod_{i=1}^{N} \\mathcal{N}(x_i | \\mu, \\sigma), \\\\\n",
    "p(\\alpha) &=& \\mathcal{N}(\\mu_\\mu, \\sigma_\\mu)\\, \\mathcal{HN}(0, \\sigma_\\sigma),\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where now $\\mathcal{D}$ denotes the whole dataset, $\\alpha$ denotes the set of parameters and we assumed that the datapoints are independent and identically distributed and that the parameters are independent as well.\n",
    "\n",
    "The sampling process generating a value $x$ is now to be understood as follows: first the priors are sampled and values for $(\\mu, \\sigma)$ are generated. Then those values are used as the parameters for the Gaussian distribution that is sampled to generate $x$. We now want to genereate `N` such points with TFP. Then, we'll write down various ways to code up the joint probability distribution so that, given a set of points and values for the parameters, it returns a number (their joint probability). That's what numerical methods require."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0687231-2df3-4dd0-b862-8cd1355f808c",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6d2c5-0163-4cab-9c85-8797de1e64e8",
   "metadata": {},
   "source": [
    "First, we need to generate some datapoints. This entails some work that already goes in the direction of writing the joint probability distribution, but we'll also revisit that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585fe2c-52ad-4ccb-9fda-e5aeb7f78d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f2917-91a3-4cfa-9a3a-273d68b80d51",
   "metadata": {},
   "source": [
    "Structure of the joint probability distribution: a single distribution for $\\mu$ and a single distribution for $\\sigma$ corresponds to `n_samples` independent copies of the distribution of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43034219-c21c-45b1-b7bf-4ceec98bc350",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "\n",
    "μ_μ = 15. \n",
    "σ_μ = 2.\n",
    "\n",
    "σ_σ = 1.\n",
    "\n",
    "distr_dict = {\n",
    "    'μ': tfd.Normal(loc=μ_μ, scale=σ_μ),\n",
    "    'σ': tfd.HalfNormal(scale=σ_σ),\n",
    "    # n_samples independent Gaussian distribution.\n",
    "    'x': lambda μ, σ: tfd.Independent(\n",
    "        # The following dimension expansion allows for broadcasting the values of\n",
    "        # μ and σ (if not done, there would be different behaviours in sampling\n",
    "        # when calling for a single sample vs multiple samples.\n",
    "        tfd.Normal(loc=tf.expand_dims(μ, -1) * tf.ones(n_samples), scale=tf.expand_dims(σ, -1) * tf.ones(n_samples)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "}\n",
    "\n",
    "generating_distr = tfd.JointDistributionNamed(\n",
    "    distr_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093938e-838a-41c7-8e03-c9cd668923e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c23e8-75bf-4cec-9fee-f524235edc54",
   "metadata": {},
   "source": [
    "Note: because of the structure, each sample will consist in a signle sample of $\\mu$, a single sample of $\\sigma$ and 100 `n_samples` samples of $x$.\n",
    "\n",
    "Example: if we sample the joint probability distribution 1000 times we'll get two tensors of 1000 samples each for $\\mu$ and $\\sigma$ respectively, and a tensor of shape `(100, 1000)` of samples of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb8571-beff-4b81-8739-b50f257fe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = generating_distr.sample(sample_shape=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764de7b-f682-42c7-ba00-03023548d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f921c5-b38f-4474-8e04-895ba56fa763",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x'][:, 0].numpy().mean(), samples['μ'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67c929-666b-4eda-a28f-66b0085bc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['x'][:, 1].numpy().mean(), samples['μ'][1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6083f-93e7-4b1e-854c-fc90251dc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 6))\n",
    "\n",
    "plt.subplots_adjust(wspace=.4)\n",
    "\n",
    "for i, rv in enumerate(samples.keys()):\n",
    "    sns.histplot(\n",
    "        x=samples[rv].numpy().ravel(),\n",
    "        ax=axs[i],\n",
    "        stat='density',\n",
    "        kde=True\n",
    "    )\n",
    "    plt.sca(axs[i])\n",
    "    plt.title(f'{rv} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d467df89-6b7a-45ba-8971-04ee824db97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_samples = {\n",
    "    'μ': samples['μ'][0],  # 1 value.\n",
    "    'σ': samples['σ'][0],  # 1 value.\n",
    "    'x': samples['x'][0, :]  # n_samples values.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1af2b-3667-426c-879b-286267c1e0c2",
   "metadata": {},
   "source": [
    "Passing a \"single sample\" to the `log_prob` method of the joint probability distribution object, the log (joint) probability of the sample (i.e. one value for each distribution in the joint distribution) is computed. This corresponds to the sum of the log probabilities obtained from each \"component\" distribution.\n",
    "\n",
    "**Note:** 1 \"sample\" actually corresponds to 1 value for $\\mu$ and $\\sigma$ respectively and 100 values for $x$. The result will be the joint probability of that sample (1 value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417e97e5-a823-4a97-8383-8ccb430962b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_distr.log_prob(single_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b293f48-4db6-4148-bb57-bf7edfacef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([\n",
    "    tfd.Normal(loc=μ_μ, scale=σ_μ).log_prob(single_samples['μ']).numpy(),\n",
    "    tfd.HalfNormal(scale=σ_σ).log_prob(single_samples['σ']).numpy(),\n",
    "    tfd.Independent(\n",
    "        tfd.Normal(loc=[single_samples['μ']] * n_samples, scale=[single_samples['σ']] * n_samples),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    ).log_prob(single_samples['x']).numpy()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168a2f5-c8dc-4864-af49-828d08f3c10e",
   "metadata": {},
   "source": [
    "We can also compute the log prob of multiple samples at the same time just by passing tensors with the appropriate shape (notice that the sample size should correspond to the **first axis**). In this case we'll get one value of the log prob for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91e6ac-bef0-431c-af4b-70152fb5ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "generating_distr.log_prob(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad35a27-b46b-433b-a85f-b6a5884c4204",
   "metadata": {},
   "source": [
    "## Sampling the hierarchical model VS sampling with a single value for each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c39c6-dfa4-4e8a-afb8-c8891dd5c96d",
   "metadata": {},
   "source": [
    "This is equivalent to having Dirac deltas a priors for the parameter of the Gaussian generating $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae98f83-47d5-49b5-996f-8e7f2f97e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample di distributions.\n",
    "N = 10000\n",
    "\n",
    "samples = generating_distr.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2322c27-154b-491e-bac4-054921b922ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(14, 6))\n",
    "\n",
    "plt.subplots_adjust(wspace=.4)\n",
    "\n",
    "for i, rv in enumerate(samples.keys()):\n",
    "    sns.histplot(\n",
    "        x=samples[rv].numpy().ravel(),\n",
    "        ax=axs[i],\n",
    "        stat='density',\n",
    "        kde=True\n",
    "    )\n",
    "    plt.sca(axs[i])\n",
    "    plt.title(f'{rv} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a51be-8103-440b-bca8-e842e908de7f",
   "metadata": {},
   "source": [
    "In the data generation process, the distributions were sampled hieararchically: first the distributions for $\\mu$ and $\\sigma$ were sampled, then these values were used as the parameters to define the Gaussian distribution from which to sample $x$ - and this was repeated for each value of $x$.\n",
    "\n",
    "What if we didn't sample $\\mu$ and $\\sigma$ but just used a single value for each? We can have a look at what would have happened by sampling a Gaussian distribution for $x$ with values for the parameters fixed at the mean value of the sampled $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fcaec-9775-4973-8b00-2da844716695",
   "metadata": {},
   "outputs": [],
   "source": [
    "μ_mean = tf.reduce_mean(samples['μ'])\n",
    "σ_mean = tf.reduce_mean(samples['σ'])\n",
    "\n",
    "print(f'Mean μ: {μ_mean} | Mean σ: {σ_mean}')\n",
    "\n",
    "x_samples_single_parameters = tfd.Normal(loc=μ_mean, scale=σ_mean).sample(N)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=samples['x'].numpy().ravel(),\n",
    "    stat='density',\n",
    "    kde=True,\n",
    "    label='Samples from hierarchical model',\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "sns.histplot(\n",
    "    x=x_samples_single_parameters,\n",
    "    stat='density',\n",
    "    kde=True,\n",
    "    label='Samples with mean values for the parameters',\n",
    "    color=sns.color_palette()[1]\n",
    ")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Comparison between sampling the full model and using only the mean values for the parameters', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c200eedf-1e79-44e3-b9f2-37f3f3c010b7",
   "metadata": {},
   "source": [
    "Is the result from the hierarchical model still a Gaussian? Let's try to fit one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95612537-b569-44da-b0b6-503c056d5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "sns.histplot(\n",
    "    x=samples['x'].numpy().ravel(),\n",
    "    stat='density',\n",
    "    kde=True,\n",
    "    label='Samples from hierarchical model',\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "x_range = tf.linspace(samples['x'].numpy().min(), samples['x'].numpy().max(), 1000)\n",
    "\n",
    "sns.lineplot(\n",
    "    x=x_range,\n",
    "    y=tfd.Normal(loc=samples['x'].numpy().mean(), scale=samples['x'].numpy().std()).prob(x_range),\n",
    "    label='\"Fitted\" Gaussian',\n",
    "    color=sns.color_palette()[1]\n",
    ")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Comparison with a Gaussian with parameters estimated from the samples', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de658a3-6d8c-45a1-9758-a4135ac71c7e",
   "metadata": {},
   "source": [
    "## Writing joint probaiblities with `JointDistributionCoroutine`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adfdbb4-f7ff-4bbb-8132-f6f59b95cea7",
   "metadata": {},
   "source": [
    "TFP offers different methods to define joint probability distribution, one of them is `JointProbabilityNamed` (seen above) and another ont is `JointDistributionCoroutine`, which can be made to act as a function decorator turning the function into a distribution object.\n",
    "\n",
    "Inside the function, the component distributions must be specified using the `yield` keyword, with the roots of the Bayesian graphs identified with `Root`. The result is equivalent to the one seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9cb7d6-d3bc-4ea1-a2fb-b8f9777a7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tfd.JointDistributionCoroutine\n",
    "def generating_distr_jdc():\n",
    "    μ = yield tfd.JointDistributionCoroutine.Root(tfd.Normal(loc=μ_μ, scale=σ_μ))\n",
    "    σ = yield tfd.JointDistributionCoroutine.Root(tfd.HalfNormal(scale=σ_σ))\n",
    "    x = yield tfd.Independent(\n",
    "        # The following dimension expansion allows for broadcasting the values of\n",
    "        # μ and σ (if not done, there would be different behaviours in sampling\n",
    "        # when calling for a single sample vs multiple samples.\n",
    "        # tfd.Normal(loc=tf.expand_dims(μ, -1) * tf.ones(n_samples), scale=tf.expand_dims(σ, -1) * tf.ones(n_samples)),\n",
    "        tfd.Normal(loc=tf.expand_dims(μ, -1) * tf.ones(n_samples), scale=tf.expand_dims(σ, -1) * tf.ones(n_samples)),\n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "    \n",
    "generating_distr_jdc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
