{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadf3979-f75c-4f3b-8e6b-7ad933984578",
   "metadata": {},
   "source": [
    "## CNNs with MC dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6394a-cd2f-41e1-ac19-386d7d87e611",
   "metadata": {},
   "source": [
    "MC dropout is a technique that exploits the presence of dropout layers in a NN (usually used for regularization), keeping their effect active at inference time as well as at training time (which is usually not the case). This can be interpreted as giving rise to a Bayesian NN in which each node affected by the dropout layers is replaced by a Bernoulli distribution - much the same way in which nodes were replaced by a Gaussian approximate posterior when using reparametrization or flipout layers.\n",
    "\n",
    "__Classic idea of dropout:__\n",
    "- At _training time_, at each iteration in the training loop each node affected by a dropout layer is set to 0 with probability $p$ (independent for each node). At each iteration the model is effectively different, as some of the connections between nodes have been cut. This helps preventing overfitting because the network needs to learn how to correctly connect input and output wihtout relying always on the same nodes: no individual node (or sub-network) is responsible for a particular prediction. Backpropagation happens across all active nodes.\n",
    "- At _inference time_, all nodes are kept active with a fixed value (dropout is turned off). If the value learned during training is $w$, though, the value actually used is set to be $w^\\star = w\\, p$, reflecting that due to the dropout procedure each (affected) node has seen the data a fraction $p$ times the number of training loops.\n",
    "\n",
    "__MC dropout__:\n",
    "- At _training time_ everything remains the same as with classic dropout.\n",
    "- At _inference time_ **dropout is kept active** so that each prediction effectively sees a different NN. As with probabilistic deep learning, multiple predictions over the same inputs generate different outputs (each time a different set of weights is kept active): this is indeed another example of Bayesian NN, with a Bernoulli (discrete) probability distribution over the weights. For each weight affected by dropout the distribution is\n",
    "$$\n",
    "w^\\star = \\left\\lbrace\n",
    "\\begin{array}{l}\n",
    "w\\quad\\text{with probability $p$}\\,,\\\\\n",
    "0\\quad\\text{with probability $1 - p$}\\,.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Technically this is very easy to achieve: with Keras it's sufficient to use the model in \"training mode\" (the model state corresponding to active dropout) also at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9c847-3a46-480b-827e-431c90563612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Dropout,\n",
    "    BatchNormalization, Dense, Activation, Flatten)\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "from tensorflow.keras import Input, Model\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade5c9ba-4bae-46ad-aa6f-4c4c469fbb81",
   "metadata": {},
   "source": [
    "## Load data: the CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc946f-d911-4685-b8ec-123ccb8b8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, n_classes=None, pixel_norm=255.):\n",
    "    \"\"\"\n",
    "    Preprocesses data.\n",
    "    \"\"\"\n",
    "    # Turn images to grayscale.\n",
    "    x_preprocessed = keras_cv.layers.Grayscale()(x)\n",
    "\n",
    "    # Normalize pixel values.\n",
    "    x_preprocessed = x_preprocessed / pixel_norm\n",
    "\n",
    "    # Convert to Tensorflow tensor.\n",
    "    y_preprocessed = tf.constant(y[:, 0], dtype=tf.int32)\n",
    "\n",
    "    # One-hot encode the true labels.\n",
    "    if n_classes is None:\n",
    "        print('Inferring the number of classes', end='')\n",
    "        \n",
    "        depth = tf.reduce_max(y_preprocessed) + 1\n",
    "\n",
    "        print(f' | {depth} classes found')\n",
    "    else:\n",
    "        depth = n_classes\n",
    "\n",
    "    y_preprocessed = tf.one_hot(y_preprocessed, depth)\n",
    "\n",
    "    return x_preprocessed, y_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b6200-9135-433e-90b9-fcd7a206f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train_raw.shape, y_train_raw.shape, x_test_raw.shape, y_test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32397a2-eb3e-40eb-9875-709282eb7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = preprocess_data(x_train_raw, y_train_raw)\n",
    "x_test, y_test = preprocess_data(x_test_raw, y_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f553a3-ee07-4425-b98c-dd36564d33e5",
   "metadata": {},
   "source": [
    "## Build and train a model with dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7807a-0d50-4818-a8af-6f281d342641",
   "metadata": {},
   "source": [
    "__Idea:__ build a traditional CNN model and train it, then define the corresponding MC dropout model using the same (trained) layers, but in which the dropout layers are called with the `training=True` option.\n",
    "\n",
    "__Notes:__\n",
    "- The above idea needs the MC dropout model to be built using Keras' functional API, as we need to specify the `training=True` option right inside the call to the droppout layers.\n",
    "- We could have accessed the Keras backend using the `tensorflow.keras.backend.set_learning_phase`, but that would have influenced **all** the layers at the same time (even the `BatchNormalization` ones, which also have a different behovious between training and inference time and should be called with `training=False` at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac5767-5116-4337-8b1d-32a24163bc9b",
   "metadata": {},
   "source": [
    "### Build and train a CNN with dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dca63-9454-4b56-8e02-12a51b91cbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Conv block.\n",
    "    Conv2D(filters=8, kernel_size=(3, 3), padding='same'),\n",
    "    Activation(relu),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Conv block.\n",
    "    Conv2D(filters=16, kernel_size=(3, 3), padding='same'),\n",
    "    Activation(relu),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    # Flatten.\n",
    "    Flatten(),\n",
    "    # Dense block.\n",
    "    Dense(64),\n",
    "    Activation(relu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=0.9),\n",
    "    Dense(64),\n",
    "    Activation(relu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(rate=0.9),\n",
    "    # Output layer.\n",
    "    Dense(10),\n",
    "    Activation(softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7492b61-3e7c-482e-8a6e-69f31f4ce6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model(x_train[:1, ...])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e79a-0659-4bfe-b543-386bcfcacfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size=128\n",
    "\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3e712-3007-4a20-b6d2-792adc7bedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "\n",
    "models_dir = '../models/'\n",
    "\n",
    "if save_model:\n",
    "    model.save(os.path.join(models_dir, 'dropout_cnn.tf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a04120-a605-4539-83e7-e8f66014ae59",
   "metadata": {},
   "source": [
    "### Build the equivalent MC dropout CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654d228-70ee-44d6-9f2e-ed057c7470ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropoutModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Given a model with dropout layers (passed to the\n",
    "    constructor), this class builds an equivalent model\n",
    "    with the same exact layers (with trained parameters),\n",
    "    but in which the dropout layers are always called with\n",
    "    the `training` option set to True so the sampling\n",
    "    happens at inference time as well.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_model):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        input = Input(shape=(32, 32, 1,))\n",
    "\n",
    "        output = self.original_model.layers[0](input)\n",
    "\n",
    "        for layer in self.original_model.layers[1:]:\n",
    "            if 'dropout' in layer.name:\n",
    "                print(f'Dropout layer found: {layer.name}')\n",
    "                \n",
    "                output = layer(output, training=True)\n",
    "            else:\n",
    "                output = layer(output)\n",
    "\n",
    "        return Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1534d8-9f22-40a4-a873-1ddd912048bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_dropout_model = MCDropoutModel(model).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08bbd3-d279-47dc-9b64-17ce398336a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate `n_pred` predictions for a single sample.\n",
    "n_pred = 100\n",
    "\n",
    "pred = []\n",
    "\n",
    "for _ in range(n_pred):\n",
    "    pred.append(mc_dropout_model(x_test[:1, ...]))\n",
    "\n",
    "pred = tf.concat(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546d315-3a39-43fe-b0df-9da275be820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of predicted probabilities\n",
    "# for each class.\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "for class_index in range(pred.shape[1]):\n",
    "    sns.scatterplot(\n",
    "        x=[class_index] * pred.shape[0],\n",
    "        y=pred[:, class_index],\n",
    "        alpha=.5,\n",
    "        color=sns.color_palette()[0]\n",
    "    )\n",
    "\n",
    "plt.xticks(\n",
    "    ticks=range(pred.shape[1]),\n",
    ")\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Predicted probability')\n",
    "plt.title('Distribution of predicted probabilities for each class (1 sample)', fontsize=14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
